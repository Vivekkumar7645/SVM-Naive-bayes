{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q 1.What is a Support Vector Machine (SVM)?\n",
        "**Ans** - A Support Vector Machine is a supervised machine learning algorithm used for classification and regression tasks. It is particularly effective for high-dimensional datasets and works well when there is a clear margin of separation between different classes.\n",
        "\n",
        "**SVM Working**\n",
        "1. Finding the Optimal Hyperplane\n",
        "  * SVM aims to find the best decision boundary that separates data points of different classes with the maximum margin.\n",
        "  * The margin is the distance between the hyperplane and the closest data points.\n",
        "\n",
        "2. Support Vectors\n",
        "  * The data points that are closest to the hyperplane and influence its position are called support vectors.\n",
        "  * These points help define the decision boundary.\n",
        "\n",
        "3. Handling Non-Linearly Separable Data\n",
        "  * If the data is not linearly separable, SVM uses a kernel trick to transform the data into a higher-dimensional space where it becomes linearly separable.\n",
        "  * Common kernel functions include:\n",
        "    * Linear Kernel: Used for linearly separable data.\n",
        "    * Polynomial Kernel: Maps data into a polynomial feature space.\n",
        "    * Radial Basis Function (RBF) Kernel: Handles more complex, non-linear relationships.\n",
        "    * Sigmoid Kernel: Similar to a neural network activation function.\n",
        "\n",
        "**Use Cases**\n",
        "* Text classification (e.g., spam detection)\n",
        "* Image recognition\n",
        "* Bioinformatics (e.g., protein classification)\n",
        "* Fraud detection\n",
        "\n",
        "**Advantages of SVM**\n",
        "* Works well in high-dimensional spaces\n",
        "* Effective for both linear and non-linear classification\n",
        "* Robust to overfitting, especially with proper kernel selection\n",
        "* Memory efficient since it only relies on a subset of training data (support vectors)\n",
        "\n",
        "**Disadvantages of SVM**\n",
        "* Computationally expensive for large datasets\n",
        "* Choosing the right kernel and hyperparameters can be tricky\n",
        "* Sensitive to noise in overlapping class distributions"
      ],
      "metadata": {
        "id": "N6DX8_73zbvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "**Ans** - The difference between Hard Margin SVM and Soft Margin SVM lies in how they handle misclassified data points and how strictly they enforce separation between classes.\n",
        "\n",
        "**1. Hard Margin SVM**\n",
        "\n",
        "Strict separation\n",
        "* Used when the data is perfectly linearly separable.\n",
        "* The SVM tries to find a maximum-margin hyperplane that strictly separates all data points.\n",
        "* It does not allow any misclassification, meaning every point must be on the correct side of the margin.\n",
        "* It minimizes the margin while ensuring no violations of the separation constraint.\n",
        "\n",
        "**Limitations:**\n",
        "* Highly sensitive to noise: Even a small outlier can drastically change the decision boundary.\n",
        "* Not useful for non-linearly separable data, as it requires a perfect separation.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "    min(1/2)||w||²\n",
        " Subject to:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1, ∀ᵢ\n",
        "(where 'w' is the weight vector, b is the bias, and yᵢ is the class label of point xᵢ)\n",
        "\n",
        "**2. Soft Margin SVM**\n",
        "\n",
        "Allows some misclassification\n",
        "* Used when the data is not perfectly separable.\n",
        "* Instead of strictly separating classes, it allows some data points to be on the wrong side of the margin.\n",
        "* Introduces a slack variable to allow violations and control the trade-off between maximizing the margin and minimizing classification error.\n",
        "* The regularization parameter C controls how much misclassification is allowed:\n",
        "  * High C → Less tolerance for misclassification.\n",
        "  * Low C → More tolerance for misclassification, leading to better generalization.\n",
        "\n",
        "**Advantages:**\n",
        "* Works well with noisy and overlapping data.\n",
        "* Can handle outliers better than hard margin SVM.\n",
        "* Suitable for real-world applications where data is rarely perfectly separable.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "    min(1/2)||w||²+C∑ⁿᵢ₌₁ξᵢ\n",
        "\n",
        "Subject to:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1−ξᵢ, ξᵢ ≥ 0, ∀ᵢ\n",
        "where ξᵢ are the slack variables allowing misclassification\n",
        "\n",
        "**Differences**\n",
        "\n",
        "|Feature\t|Hard Margin SVM\t|Soft Margin SVM|\n",
        "|-|||\n",
        "|Misclassification\t|Not allowed\t|Allowed (controlled by C)|\n",
        "|Use case\t|Perfectly separable data\t|Noisy or overlapping data|\n",
        "|Outlier sensitivity\t|High\t|Lower |\n",
        "|Regularization parameter (C)\t|Not needed\t|Needed to balance margin width vs. misclassification|"
      ],
      "metadata": {
        "id": "x0kZNRzhzfyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 3. What is the mathematical intuition behind SVM?\n",
        "**Ans** - The mathematical intuition behind Support Vector Machines revolves around finding the optimal hyperplane that maximizes the margin between two classes.\n",
        "\n",
        "**1. The Decision Hyperplane**\n",
        "\n",
        "Given a dataset of n points:\n",
        "\n",
        "    (x₁,y₁),(x₂,y₂),…,(xₙ,yₙ)\n",
        "where:\n",
        "* xᵢ ∈ Rᵈ is a feature vector.\n",
        "* yᵢ ∈ {−1,1} is the class label.\n",
        "\n",
        "A hyperplane is defined as:\n",
        "\n",
        "    w⋅x + b = 0\n",
        "where:\n",
        "* w is the weight vector.\n",
        "* b is the bias.\n",
        "* x is the input data point.\n",
        "\n",
        "A point x is classified based on:\n",
        "\n",
        "    ŷ = sign(w⋅x+b)\n",
        "\n",
        "**2. Margin and Optimal Hyperplane**\n",
        "\n",
        "**Geometric Margin**\n",
        "\n",
        "For a correctly classified point:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1\n",
        "The margin is the perpendicular distance from a data point to the hyperplane, given by:\n",
        "\n",
        "    |w⋅x+b|/||w||\n",
        "The total margin is defined as the distance between the closest positive and negative samples to the hyperplane. The goal of SVM is to maximize this margin.\n",
        "\n",
        "**Optimization Problem**\n",
        "\n",
        "To maximize the margin, we minimize ||w||, subject to the constraint:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1,∀ᵢ\n",
        "This leads to the primal optimization problem:\n",
        "\n",
        "    minᵥᵥ,₆ (1/2)||w||²\n",
        "subject to:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1,∀ᵢ\n",
        "where (1/2)||w||² is used instead of||w|| for mathematical convenience.\n",
        "\n",
        "**3. Introducing Slack Variables for Soft Margin SVM**\n",
        "\n",
        "When data is not perfectly separable, we introduce slack variables ξi to allow misclassification:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1−ξᵢ, ξᵢ ≥ 0\n",
        "The new objective function becomes:\n",
        "\n",
        "    minᵥᵥ,₆ (1/2)||w||² + C∑ⁿᵢ₌₁ξᵢ\n",
        "\n",
        "where 'C' controls the trade-off between maximizing the margin and allowing misclassifications.\n",
        "\n",
        "**4. The Dual Form and Kernel Trick**\n",
        "\n",
        "To solve this problem efficiently, we convert it to its dual form using Lagrange multipliers:\n",
        "\n",
        "    max α ∑ⁿᵢ₌₁ αᵢ−(1/2)∑ⁿᵢ₌₁∑ⁿⱼ₌₁ αᵢαⱼyᵢyⱼ(xᵢ⋅xⱼ)\n",
        "subject to:\n",
        "\n",
        "    ∑ⁿᵢ₌₁ αᵢyᵢ = 0, 0≤αᵢ≤C\n",
        "This dual form allows us to use the kernel trick, where we replace (xᵢ⋅xⱼ) with a kernel function K(xᵢ,xⱼ), enabling SVM to work with non-linearly separable data.\n",
        "\n",
        "Common kernels:\n",
        "* Linear Kernel: K(xᵢ,xⱼ) = xᵢ⋅xⱼ\n",
        "* Polynomial Kernel: K(xᵢ,xⱼ) = (xᵢ⋅xⱼ+c)ᵈ\n",
        "* RBF Kernel: K(xᵢ,xⱼ) = exp(-γ||xᵢ-xⱼ||²)\n",
        "\n",
        "**5. Final Decision Function**\n",
        "\n",
        "Once we solve for α, the final decision function is:\n",
        "\n",
        "    f(x) = ∑ⁿᵢ₌₁ αᵢyᵢK(xᵢ,x)+b\n",
        "where only support vectors (points where αᵢ>0) contribute to the decision boundary."
      ],
      "metadata": {
        "id": "rJaOkoPRzhAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 4. What is the role of Lagrange Multipliers in SVM?\n",
        "**Ans** - **Role of Lagrange Multipliers in SVM**\n",
        "\n",
        "Lagrange multipliers play a crucial role in Support Vector Machines by transforming the constrained optimization problem into an easier-to-solve dual problem. This transformation enables the efficient computation of the optimal hyperplane and allows the use of kernel functions for non-linearly separable data.\n",
        "\n",
        "**1. Use Lagrange Multipliers**\n",
        "\n",
        "The original SVM optimization problem involves constraints:\n",
        "\n",
        "    minᵥᵥ,₆ (1/2)||w||²\n",
        "subject to:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1, ∀ᵢ\n",
        "This is a constrained optimization problem, which is difficult to solve directly. To handle such constraints, we use the Lagrangian function.\n",
        "\n",
        "**2. Primal Formulation with Lagrange Multipliers**\n",
        "\n",
        "We introduce Lagrange multipliers\n",
        "αᵢ to convert the constrained problem into an unconstrained optimization problem:\n",
        "\n",
        "**Lagrangian Function:**\n",
        "\n",
        "    L(w,b,α) = (1/2)||w||² - ∑ⁿᵢ₌₁ αᵢ[yᵢ(w⋅xᵢ+b)−1]\n",
        "where:\n",
        "* αᵢ ≥ 0 are the Lagrange multipliers.\n",
        "* If a constraint is not satisfied, αᵢ increases, forcing the optimization to respect the constraint.\n",
        "\n",
        "To find the optimal w and b, we take the partial derivatives of 'L' and set them to zero:\n",
        "\n",
        "1. Derivative with respect to w:\n",
        "\n",
        "        ∂L/∂w = w-∑ⁿᵢ₌₁ αᵢyᵢxᵢ = 0\n",
        "Solving for w:\n",
        "\n",
        "        w = ∑ⁿᵢ₌₁αᵢyᵢxᵢ\n",
        "This shows that only support vectors (where αᵢ>0) define the decision boundary.\n",
        "\n",
        "2. Derivative with respect to b:\n",
        "\n",
        "        ∑ⁿᵢ₌₁ αᵢyᵢ = 0\n",
        "This ensures that the classification boundary is unbiased.\n",
        "\n",
        "**3. Dual Formulation**\n",
        "\n",
        "By substituting w into the Lagrangian function and eliminating w and b, we get the dual optimization problem:\n",
        "\n",
        "    max α ∑ⁿᵢ₌₁ αᵢ-(1/2)∑ⁿᵢ₌₁∑ⁿⱼ₌₁ αᵢαⱼyᵢyⱼ(xᵢ⋅xⱼ)\n",
        "subject to:\n",
        "\n",
        "    ∑ⁿᵢ₌₁ αᵢyᵢ=0, 0≤αᵢ≤C\n",
        "Here:\n",
        "* αᵢ  represents the importance of each training point.\n",
        "* Only support vectors have αᵢ>0.\n",
        "* Non-support vectors have αᵢ=0, meaning they do not influence the decision boundary.\n",
        "\n",
        "**4. Role of Lagrange Multipliers in the Kernel Trick**\n",
        "\n",
        "In non-linearly separable data, we use the kernel trick by replacing (xᵢ⋅xⱼ) with a kernel function K(xᵢ,xⱼ):\n",
        "\n",
        "    max α ∑ⁿᵢ₌₁ αᵢ-(1/2)∑ⁿᵢ₌₁∑ⁿⱼ₌₁ αᵢαⱼyᵢyⱼK(xᵢ,xⱼ)\n",
        "This allows SVM to work in higher-dimensional feature spaces without explicitly computing the transformation.\n",
        "\n",
        "**5. Interpretation of Lagrange Multipliers**\n",
        "* If αᵢ=0, the point is far from the margin and does not affect the decision boundary.\n",
        "* If 0 <αᵢ< C, the point is on the margin.\n",
        "* If αᵢ = C, the point is inside the margin."
      ],
      "metadata": {
        "id": "vFl5hx2jzo5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 5. What are Support Vectors in SVM?\n",
        "**Ans** - **Support Vectors in SVM**\n",
        "\n",
        "Support Vectors are the data points that lie closest to the decision boundary and play a crucial role in defining it. They are the most influential points in Support Vector Machines because they determine the position and orientation of the optimal hyperplane.\n",
        "\n",
        "**1. Support Vectors are Important**\n",
        "* They define the margin: The SVM algorithm maximizes the distance between the hyperplane and the closest data points.\n",
        "* Only these points affect the decision boundary: Other training points do not influence the model directly.\n",
        "* They make SVM memory efficient: Since only a small subset of data points is used for classification, SVM does not need to store all training points after training.\n",
        "\n",
        "**2. Support Vectors are Identified**\n",
        "\n",
        "The SVM optimization problem is:\n",
        "\n",
        "    min (1/2)||w||²\n",
        "subject to:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1\n",
        "The support vectors are the data points that exactly satisfy this constraint:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) = 1\n",
        "These points lie on the margin and are critical in defining the hyperplane.\n",
        "\n",
        "**3. Support Vectors in Hard Margin vs. Soft Margin SVM**\n",
        "**Hard Margin SVM**\n",
        "* Works with perfectly separable data.\n",
        "* Support vectors are the points that lie exactly on the margin.\n",
        "* No misclassified points are allowed.\n",
        "\n",
        "**Soft Margin SVM**\n",
        "* Allows some misclassification by introducing slack variablesξiξi.\n",
        "* Support vectors can be:\n",
        "  1. On the margin (0 <αᵢ< C).\n",
        "  2. Inside the margin.\n",
        "  3. Misclassified.\n",
        "\n",
        "**4. Support Vectors in the Dual Formulation**\n",
        "\n",
        "In the dual form of SVM, the optimization problem is:\n",
        "\n",
        "    max α ∑ⁿᵢ₌₁ αᵢ-(1/2)∑ⁿᵢ₌₁∑ⁿⱼ₌₁ αᵢαⱼyᵢyⱼK(xᵢ,xⱼ)\n",
        "subject to:\n",
        "\n",
        "    ∑ⁿᵢ₌₁ αᵢyᵢ = 0, 0≤αᵢ≤C\n",
        "* If αᵢ = 0 → The data point is far from the decision boundary.\n",
        "* If 0 <αᵢ< C → The data point lies on the margin.\n",
        "* If αᵢ = C → The data point is inside the margin or misclassified.\n",
        "\n",
        "Thus, support vectors are the points with nonzero Lagrange multipliers (αᵢ>0).\n",
        "\n",
        "**5. Example Visualization**\n",
        "\n",
        "Imagine a binary classification problem:\n",
        "\n",
        "* Support vectors are the bolded points on the margin."
      ],
      "metadata": {
        "id": "hng3MHhUzpsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Class +1      Class -1\n",
        "  ○ ○ ○ | ○ ● ○ | ● ● ●\n",
        "          ↑ Support Vectors"
      ],
      "metadata": {
        "id": "ogVJ3X5bymP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the middle vertical line is the optimal hyperplane, and the dashed boundaries represent the margin. The points touching the dashed lines are support vectors."
      ],
      "metadata": {
        "id": "VjDr04Fyyqbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 6. What is a Support Vector Classifier (SVC)?\n",
        "**Ans** - **Support Vector Classifier**\n",
        "\n",
        "The Support Vector Classifier is an extension of the Support Vector Machine used for classification tasks. It finds the optimal decision boundary that best separates data points into different classes while maximizing the margin and allowing some misclassifications.\n",
        "\n",
        "**1. SVC Work?**\n",
        "\n",
        "SVC aims to separate data into two classes by finding the best hyperplane:\n",
        "\n",
        "    w⋅x+b = 0\n",
        "where:\n",
        "* w is the weight vector.\n",
        "* x is the input feature vector.\n",
        "* b is the bias term.\n",
        "\n",
        "**Classification Decision Rule**\n",
        "\n",
        "A new data point x is classified as:\n",
        "\n",
        "    ŷ = sign(w⋅x+b)\n",
        "* If w⋅x+b > 0, classify as +1.\n",
        "* If w⋅x+b < 0, classify as -1.\n",
        "\n",
        "**2. Hard Margin vs. Soft Margin SVC**\n",
        "\n",
        "**Hard Margin SVC**\n",
        "* Assumes perfectly separable data.\n",
        "* No misclassifications allowed.\n",
        "* Finds the maximum-margin hyperplane.\n",
        "\n",
        "**Soft Margin SVC**\n",
        "* Allows some misclassification using slack variables ξi.\n",
        "* Introduces a regularization parameter C that balances margin maximization and misclassification.\n",
        "  * High C → Less misclassification.\n",
        "  * Low C → More flexibility, allowing some errors.\n",
        "\n",
        "**3. Mathematical Formulation**\n",
        "\n",
        "**Objective Function**\n",
        "\n",
        "    minᵥᵥ,₆ (1/2)||w||²+C∑ⁿᵢ₌₁ ξᵢ\n",
        "subject to:\n",
        "\n",
        "    yᵢ(w⋅xᵢ+b) ≥ 1−ξᵢ, ξᵢ≥0, ∀ᵢ\n",
        "where:\n",
        "* 'C' controls the trade-off between margin width and misclassification.\n",
        "* ξᵢ are slack variables allowing some misclassification.\n",
        "\n",
        "**4. Kernel Trick in SVC**\n",
        "\n",
        "For non-linearly separable data, SVC uses the kernel trick to map data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "Common kernels:\n",
        "* Linear Kernel: K(xᵢ,xⱼ) = xᵢ⋅xⱼ\n",
        "* Polynomial Kernel: K(xᵢ,xⱼ) = (xᵢ⋅xⱼ+c)ᵈ\n",
        "* Radial Basis Function Kernel:K(xᵢ,xⱼ) = exp(-γ||xᵢ-xⱼ||²)\n",
        "\n",
        "**5. Support Vector Classifier in Python (Example)**"
      ],
      "metadata": {
        "id": "aOJIEBu0zqKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_redundant=0, n_clusters_per_class=1)\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "model = SVC(kernel='linear', C=1.0)\n",
        "model.fit(X, y)\n",
        "\n",
        "w = model.coef_[0]\n",
        "b = model.intercept_[0]\n",
        "x_plot = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
        "y_plot = -(w[0] * x_plot + b) / w[1]\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
        "plt.plot(x_plot, y_plot, 'k-')\n",
        "plt.title(\"Support Vector Classifier (Linear SVM)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U13cqamz5tvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 7. What is a Support Vector Regressor (SVR)?\n",
        "**Ans** - **Support Vector Regressor**\n",
        "\n",
        "The Support Vector Regressor is an extension of Support Vector Machines for regression tasks. Unlike Support Vector Classifier, which classifies data into discrete categories, SVR predicts continuous values while maintaining the principles of maximizing the margin.\n",
        "\n",
        "**1. SVR Work**\n",
        "\n",
        "SVR aims to find a function f(x) that predicts y with minimal error while maintaining a margin ϵ:\n",
        "\n",
        "    f(x) = w⋅x+b\n",
        "where:\n",
        "* w is the weight vector.\n",
        "* x is the input feature vector.\n",
        "* b is the bias term.\n",
        "\n",
        "**The ϵ-Insensitive Tube**\n",
        "* Unlike standard regression, SVR does not minimize absolute or squared errors.\n",
        "* Instead, it defines a margin of width ϵ around the predicted function.\n",
        "* Points inside this margin are ignored.\n",
        "* Points outside the margin contribute to the loss function.\n",
        "\n",
        "This makes SVR robust to small fluctuations in data.\n",
        "\n",
        "**2. Mathematical Formulation**\n",
        "\n",
        "**Objective Function**\n",
        "\n",
        "    minᵥᵥ,₆ (1/2)||w||² + C∑ⁿᵢ₌₁(ξᵢ+ξ∗ᵢ)\n",
        "subject to:\n",
        "\n",
        "    yᵢ-(w⋅xᵢ+b) ≤ ϵ+ξᵢ\n",
        "    (w⋅xᵢ+b)-yᵢ ≤ ϵ+ξ∗ᵢ\n",
        "    ξᵢ,ξ∗ᵢ ≥ 0\n",
        "where:\n",
        "* ϵ controls the tolerance: Larger ϵ → More data points ignored.\n",
        "* C is the regularization parameter: Larger C\n",
        "C → More penalty for violating the margin.\n",
        "* ξᵢ,ξ∗ᵢ  are slack variables for handling points outside the margin.\n",
        "\n",
        "**3. Types of SVR**\n",
        "1. Linear SVR\n",
        "* Uses a straight-line regression.\n",
        "* Works when data has a linear trend.\n",
        "\n",
        "2. Non-Linear SVR\n",
        "* Uses the kernel trick to map data into higher dimensions.\n",
        "* Common kernels:\n",
        "  * Polynomial Kernel:K(xᵢ,xⱼ) = (xᵢ⋅xⱼ+c)ᵈ\n",
        "* Radial Basis Function Kernel: K(xᵢ,xⱼ) = exp(-γ||xᵢ-xⱼ||²)\n",
        "* Sigmoid Kernel: K(xᵢ,xⱼ) = tanh(γxᵢ⋅xⱼ+c)\n",
        "\n",
        "**4. SVR in Python (Example)**"
      ],
      "metadata": {
        "id": "Sw1j0HPAzqd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "svr_rbf = SVR(kernel='rbf', C=100, epsilon=0.1, gamma=0.1)\n",
        "svr_rbf.fit(X, y)\n",
        "\n",
        "y_pred = svr_rbf.predict(X)\n",
        "\n",
        "plt.scatter(X, y, label=\"Data\", color=\"blue\", s=10)\n",
        "plt.plot(X, y_pred, label=\"SVR Prediction\", color=\"red\")\n",
        "plt.legend()\n",
        "plt.title(\"Support Vector Regression (SVR) with RBF Kernel\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OauCNmZs8y4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Parameters in SVR**\n",
        "1. C\n",
        "* High C → Tries to fit all points.\n",
        "* Low C → More flexible model, ignores some points.\n",
        "\n",
        "2. ϵ\n",
        "* High ϵ → More generalization.\n",
        "* Low ϵ → More sensitive to data points.\n",
        "\n",
        "3. Kernel Type\n",
        "* Linear → Simple trends.\n",
        "* Polynomial → Moderate complexity.\n",
        "* RBF → Highly non-linear patterns."
      ],
      "metadata": {
        "id": "lAxBb5Vr82dX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 8. What is the Kernel Trick in SVM?\n",
        "**Ans** - **Kernel Trick in SVM**\n",
        "\n",
        "The Kernel Trick is a mathematical technique used in Support Vector Machines to transform non-linearly separable data into a higher-dimensional space where it becomes linearly separable—without explicitly computing the transformation.\n",
        "\n",
        "**1. Need of Kernel Trick**\n",
        "\n",
        "SVMs work well with linearly separable data, but real-world datasets are often non-linearly separable.\n",
        "For example:\n",
        "\n",
        "* Non-linearly Separable Data"
      ],
      "metadata": {
        "id": "7vOyZ0jKzqw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Class +1   Class -1\n",
        "   ○ ○      ● ●\n",
        "     ○    ● ●\n",
        "      ○  ●"
      ],
      "metadata": {
        "id": "z_y-_0LP9reF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A straight-line decision boundary cannot separate these classes.\n",
        "\n",
        "Instead of directly applying a linear SVM, we map the data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "  * Kernel Trick:\n",
        "\n",
        "Instead of explicitly computing the transformation, we use a kernel function that computes the dot product in the high-dimensional space efficiently.\n",
        "\n",
        "**2. Mathematical Formulation**\n",
        "\n",
        "**SVM Decision Function**\n",
        "\n",
        "The standard SVM classification function is:\n",
        "\n",
        "    f(x) = sign(∑ⁿᵢ₌₁ αᵢyᵢ(xᵢ⋅x)+b)\n",
        "where:\n",
        "* xᵢ are support vectors,\n",
        "* αᵢ are Lagrange multipliers,\n",
        "* yᵢ are class labels,\n",
        "* x⋅xᵢ  is the dot product.\n",
        "\n",
        "**Applying the Kernel Trick**\n",
        "\n",
        "Instead of computing x⋅xᵢ directly, we replace it with a kernel function K(x,xᵢ):\n",
        "\n",
        "    f(x) = sign(∑ⁿᵢ₌₁ αᵢyᵢK(xᵢ,x)+b)\n",
        "This allows SVM to work in a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "**3. Common Kernel Functions**\n",
        "1. Linear Kernel (For Linearly Separable Data)\n",
        "\n",
        "        K(x,x')=x⋅x'\n",
        "* Equivalent to a regular dot product.\n",
        "* Used when data is already linearly separable.\n",
        "\n",
        "**Example:**"
      ],
      "metadata": {
        "id": "f6s77f5Y9w5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVC(kernel='linear')"
      ],
      "metadata": {
        "id": "tneG8Hw6HCfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Polynomial Kernel\n",
        "\n",
        "        K(x,x')=(x⋅x'+c)ᵈ\n",
        "* Maps input data to a higher-degree polynomial space.\n",
        "* c controls bias, and d is the polynomial degree.\n",
        "\n",
        "* Example:"
      ],
      "metadata": {
        "id": "qTqTYqulHFbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVC(kernel='poly', degree=3)"
      ],
      "metadata": {
        "id": "JxA5kUyqHa1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Radial Basis Function Kernel\n",
        "\n",
        "        K(x,x') = exp(-γ||x-x'||²)\n",
        "* Transforms data into an infinite-dimensional space.\n",
        "* Works well for complex patterns.\n",
        "\n",
        "* Example:"
      ],
      "metadata": {
        "id": "AlfZAN49Hfni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVC(kernel='rbf', gamma=0.1)"
      ],
      "metadata": {
        "id": "uAOCYHJPH1nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Sigmoid Kernel\n",
        "\n",
        "        K(x,x') = tanh(γx⋅x'+c)\n",
        "* Similar to activation functions in neural networks.\n",
        "* Used less frequently than RBF or Polynomial.\n",
        "\n",
        "* Example:"
      ],
      "metadata": {
        "id": "iC6KbaD6H4rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVC(kernel='sigmoid')"
      ],
      "metadata": {
        "id": "6iQix01EIFcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Kernel Trick in Python (Example)**"
      ],
      "metadata": {
        "id": "S0wJGBjJIIMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', C=1, gamma=0.5)\n",
        "svm_rbf.fit(X, y)\n",
        "\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
        "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
        "Z = svm_rbf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 20), alpha=0.75)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\")\n",
        "plt.title(\"SVM with RBF Kernel\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0fNPHvhrIR7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n",
        "**Ans** - **Comparison of Linear, Polynomial, and RBF Kernels in SVM**\n",
        "\n",
        "|Feature\t|Linear Kernel\t|Polynomial Kernel\t|RBF (Radial Basis Function) Kernel|\n",
        "|-||||\n",
        "|Formula |K(x,x') = x⋅x' |K(x,x') = (x⋅x'+c)ᵈ\t|(K(x, x') = \\exp(-\\gamma|\n",
        "|Complexity\t|Low (Fast)\t|Medium\t|High (Slower but powerful)|\n",
        "|Best for\t|Linearly separable data\t|Moderately complex data|\tHighly non-linear data|\n",
        "|Interpretability\t|Easy to understand\t|Harder to interpret\t|Hardest to interpret|\n",
        "|Hyperparameters\t|None\t|Degree d, Bias c |γ|\n",
        "|Computational Cost\t|Low\t|Medium\t|High (depends on γ)|\n",
        "|Overfitting Risk\t|Low\t|Medium (ifd is high)\t|High (if γ is too high)|\n",
        "|Flexibility\t|Low (only linear separation)\t|Medium (depends on degree)\t|High (can model complex shapes)|\n",
        "\n",
        "**1. Linear Kernel**\n",
        "\n",
        "When to Use -\n",
        "* Data is linearly separable.\n",
        "* High-dimensional data where computing other kernels is expensive.\n",
        "* Interpretability is important.\n",
        "\n",
        "**Advantages**\n",
        "* Simple and fast.\n",
        "* Less prone to overfitting.\n",
        "* Works well when the number of features is high.\n",
        "\n",
        "**Disadvantages**\n",
        "* Only works for linearly separable data.\n",
        "* Cannot capture complex relationships.\n",
        "\n",
        "**Example in Python**"
      ],
      "metadata": {
        "id": "DzTQiiLDzrCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X, y)"
      ],
      "metadata": {
        "id": "AJ3Ofi99Klha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Polynomial Kernel**\n",
        "\n",
        "When to Use -\n",
        "* Data has a moderate level of non-linearity.\n",
        "* Relationships are better modeled with polynomial functions.\n",
        "\n",
        "**Advantages**\n",
        "* Can model non-linear decision boundaries.\n",
        "* More flexible than the linear kernel.\n",
        "* Works well when there's a polynomial relationship between features.\n",
        "\n",
        "**Disadvantages**\n",
        "* Higher computational cost than a linear kernel.\n",
        "* Choosing the right degree is difficult.\n",
        "* High-degree polynomials can lead to overfitting.\n",
        "\n",
        "**Example in Python**"
      ],
      "metadata": {
        "id": "yB8Q5j4kKor6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
        "svm_poly.fit(X, y)"
      ],
      "metadata": {
        "id": "5CfpbhfQK7KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. RBF Kernel\n",
        "\n",
        "When to Use -\n",
        "* Data is highly non-linear and complex.\n",
        "* No prior knowledge about the feature relationships.\n",
        "\n",
        "**Advantages**\n",
        "* Extremely powerful for complex decision boundaries.\n",
        "* Works well with most datasets.\n",
        "* Can map data into infinite-dimensional space.\n",
        "\n",
        "**Disadvantages**\n",
        "* Higher computational cost.\n",
        "* Overfitting risk if γ is too high.\n",
        "* Less interpretable than linear models.\n",
        "\n",
        "**Example in Python**"
      ],
      "metadata": {
        "id": "Ab9Js3PGK-Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma=0.1)\n",
        "svm_rbf.fit(X, y)"
      ],
      "metadata": {
        "id": "2OXfvO_iLUWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Visual Comparison\n",
        "\n",
        "Imagine a dataset like this:\n",
        "* Non-linearly Separable Data:"
      ],
      "metadata": {
        "id": "ud-VwGO2LWvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Class +1   Class -1\n",
        "   ○ ○      ● ●\n",
        "     ○    ● ●\n",
        "      ○  ●"
      ],
      "metadata": {
        "id": "S0YlMiROLgza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each kernel produces a different decision boundary:\n",
        "* Linear Kernel: Draws a straight line, failing to separate complex data.\n",
        "* Polynomial Kernel: Draws a curved boundary.\n",
        "* RBF Kernel: Creates a highly flexible boundary, fitting complex patterns."
      ],
      "metadata": {
        "id": "vAHVnMApLjZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 10. What is the effect of the C parameter in SVM?\n",
        "**Ans** - **Effect of the C Parameter in SVM**\n",
        "\n",
        "The C parameter in Support Vector Machines controls the trade-off between maximizing the margin and minimizing classification errors. It determines how much the model penalizes misclassified points.\n",
        "\n",
        "**1. Role of C in SVM**\n",
        "* A large C → More importance on classifying every point correctly, resulting in a smaller margin.\n",
        "* A small C → More tolerance for misclassified points, leading to a wider margin.\n",
        "\n",
        "The SVM objective function is:\n",
        "\n",
        "    minᵥᵥ,₆ (1/2)||w||²+C∑ⁿᵢ₌₁ ξᵢ\n",
        "where:\n",
        "* ||w||² controls the margin width.\n",
        "* C∑ξᵢ is the penalty for misclassified points.\n",
        "* ξᵢ are slack variables for handling misclassified points.\n",
        "\n",
        "**2. Effect of C on Decision Boundary**\n",
        "1. High C\n",
        "* Forces SVM to correctly classify most points.\n",
        "* Smaller margin, leading to a more complex model.\n",
        "* Risk of overfitting.\n",
        "\n",
        "* Good for datasets with low noise.\n",
        "* Can overfit noisy data.\n",
        "\n",
        "2. Low C\n",
        "* Allows some misclassifications to get a larger margin.\n",
        "* More generalization, making it less sensitive to noise.\n",
        "* Risk of underfitting.\n",
        "\n",
        "* Good for datasets with noise or overlapping classes.\n",
        "* May misclassify some points.\n",
        "\n",
        "**3. Visual Example**\n",
        "\n",
        "**Effect of C on Decision Boundaries**\n",
        "\n",
        "|Low C (Soft Margin)\t|High C (Hard Margin)|\n",
        "|-||\n",
        "|Wide margin, good generalization.\t|Fits training data perfectly.|\n",
        "|Allows some misclassification.\t| Overfits (small margin, poor generalization).|\n",
        "\n",
        "**4. Python Example: Effect of C in SVM**"
      ],
      "metadata": {
        "id": "zorcaGNCzrRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)\n",
        "\n",
        "svm_low_C = SVC(kernel='linear', C=0.1).fit(X, y)\n",
        "svm_high_C = SVC(kernel='linear', C=100).fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "for i, (svm_model, title) in enumerate([(svm_low_C, \"Low C (Soft Margin)\"), (svm_high_C, \"High C (Hard Margin)\")]):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "\n",
        "    xlim = plt.xlim()\n",
        "    ylim = plt.ylim()\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),\n",
        "                         np.linspace(ylim[0], ylim[1], 100))\n",
        "    Z = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contour(xx, yy, Z, levels=[0], colors='red')\n",
        "\n",
        "    plt.title(title)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "15ioDalJNwDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "**Ans** - **Role of the γ Parameter in RBF Kernel SVM**\n",
        "\n",
        "In Radial Basis Function Kernel SVM, the γ parameter controls how much influence a single training example has. It determines the range of influence of each data point when computing the decision boundary.\n",
        "\n",
        "**1. Understanding γ in the RBF Kernel**\n",
        "\n",
        "The RBF kernel function is:\n",
        "\n",
        "    K(x,x') = exp(-γ||x−x′||²)\n",
        "where:\n",
        "* ||x-x'||²  is the Euclidean distance between two data points.\n",
        "* γ determines how far the influence of each training point reaches.\n",
        "\n",
        "**Effect of γ:**\n",
        "* High γ\n",
        "  * Each data point has a small area of influence.\n",
        "  * The model captures fine details but may overfit.\n",
        "* Low γ\n",
        "  * Each data point has a large area of influence.\n",
        "  * The model is smoother but may underfit.\n",
        "\n",
        "**2. Visualizing the Effect of γ**\n",
        "\n",
        "|Low γ (Underfitting)\t|Optimal γ\t|High γ (Overfitting)|\n",
        "|-|||\n",
        "|Smooth decision boundary\t|Well-generalized decision boundary\t|Fits training data perfectly|\n",
        "|Poor accuracy on complex patterns\t|Good balance between bias & variance\t|Sensitive to noise, poor generalization|\n",
        "\n",
        "**3. Choosing the Right γ**\n",
        "* Too low γ → Model is too simple.\n",
        "* Too high γ → Model memorizes training data.\n",
        "* Best γ → Achieves a balance between bias & variance.\n",
        "\n",
        "**4. Python Example: Effect of γ in RBF SVM**"
      ],
      "metadata": {
        "id": "C1D7JClSzrhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "svm_low_gamma = SVC(kernel='rbf', C=1.0, gamma=0.1).fit(X, y)\n",
        "svm_high_gamma = SVC(kernel='rbf', C=1.0, gamma=10).fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "for i, (svm_model, title) in enumerate([(svm_low_gamma, \"Low Gamma (Underfitting)\"), (svm_high_gamma, \"High Gamma (Overfitting)\")]):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "\n",
        "    xlim = plt.xlim()\n",
        "    ylim = plt.ylim()\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),\n",
        "                         np.linspace(ylim[0], ylim[1], 100))\n",
        "    Z = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contour(xx, yy, Z, levels=[0], colors='red')\n",
        "\n",
        "    plt.title(title)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NHKmUkQKQ8Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "**Ans** - The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' Theorem. It is used for classification tasks, particularly in text classification, spam filtering, and sentiment analysis.\n",
        "\n",
        "It assumes that features are independent given the class, which simplifies probability computations.\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "The term \"Naïve\" comes from its assumption that all features are independent given the class label. This is rarely true in real-world data, but the assumption makes computations much simpler and often still gives good results.\n",
        "\n",
        "For example, in email spam detection, the algorithm assumes that the presence of words like \"money\" and \"free\" in an email are independent of each other, even though in reality they often appear together.\n",
        "\n",
        "**Bayes' Theorem in Naïve Bayes**\n",
        "\n",
        "Bayes' Theorem states:\n",
        "\n",
        "    P(Y|X) = (P(X|Y)⋅P(Y)/P(X))\n",
        "Where:\n",
        "* P(Y|X) = Posterior Probability.\n",
        "* P(X|Y) = Likelihood.\n",
        "* P(Y) = Prior Probability.\n",
        "* P(X) = Evidence.\n",
        "\n",
        "Since P(X) is constant across all classes, the formula simplifies to:\n",
        "\n",
        "    P(Y|X) ∝ P(X|Y)P(Y)\n",
        "\n",
        "**Naïve Bayes Assumption**\n",
        "\n",
        "If X=(x₁,x₂,...,xₙ) represents multiple features, Naïve Bayes assumes conditional independence:\n",
        "\n",
        "    P(X|Y) = P(x₁|Y)P(x₂|Y)...P(xₙ|Y)\n",
        "Thus, the final formula for classification is:\n",
        "\n",
        "    P(Y|X) ∝ P(Y)∏ⁿᵢ₌₁ P(xᵢ|Y)\n",
        "**Types of Naïve Bayes Classifiers**\n",
        "1. Gaussian Naïve Bayes\n",
        "2. Multinomial Naïve Bayes\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "**Example: Spam Classification**\n",
        "\n",
        "Suppose we want to classify an email as spam or not spam based on words:\n",
        "\n",
        "|Word|Spam Probability P(X|Spam)|Not Spam Probability P(X|NotSpam)||------|----------------|------------------||Free|0.8|0.1||Money|0.7|0.2||Win|0.9|0.1|\n",
        "\n",
        "If we receive an email containing \"Free Money\", we compute:\n",
        "\n",
        "P(Spam|X) ∝ P(Spam) x P(Free|Spam) x P(Money|Spam)\n",
        "P(NotSpam|X) ∝ P(NotSpam) x P(Free|NotSpam) x P(Money|NotSpam)\n",
        "\n",
        "Whichever is higher determines the classification.\n",
        "\n",
        "**Advantages**\n",
        "* Fast and efficient, even on large datasets.\n",
        "* Performs well with high-dimensional data.\n",
        "* Handles missing data well.\n",
        "* Works surprisingly well even if the independence assumption is violated.\n",
        "\n",
        "**Disadvantages**\n",
        "* Feature independence assumption is unrealistic in many real-world cases.\n",
        "* Poor performance on highly correlated features.\n",
        "* Cannot capture complex relationships between features.\n",
        "\n",
        "**Python Example**"
      ],
      "metadata": {
        "id": "Q-K-p6bOzrys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "emails = [\"free money now\", \"win a lottery\", \"hello friend, how are you?\", \"urgent: win money\"]\n",
        "labels = [1, 1, 0, 1]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X, labels)\n",
        "\n",
        "new_email = [\"win free cash\"]\n",
        "X_new = vectorizer.transform(new_email)\n",
        "prediction = nb.predict(X_new)\n",
        "\n",
        "print(\"Spam\" if prediction[0] == 1 else \"Not Spam\")"
      ],
      "metadata": {
        "id": "rFba_e6NBbGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 13. What is Bayes' Theorem?\n",
        "**Ans** - **Bayes' Theorem: Understanding Conditional Probability**\n",
        "\n",
        "Bayes' Theorem is a fundamental theorem in probability theory that describes how to update our belief about an event based on new evidence. It is widely used in machine learning, statistics, and decision-making.\n",
        "\n",
        "**1. The Formula for Bayes' Theorem**\n",
        "\n",
        "    P(A|B) = (P(B|A)⋅P(A))/P(B)\n",
        "Where:\n",
        "* P(A|B) = Posterior Probability (Probability of event A occurring given that B has occurred).\n",
        "* P(B|A) = Likelihood (Probability of observing B given A is true).\n",
        "* P(A) = Prior Probability (Initial belief about A, before observing B).\n",
        "* P(B) = Marginal Probability (Total probability of B occurring, across all possible A).\n",
        "\n",
        "**2. Intuition Behind Bayes' Theorem**\n",
        "\n",
        "Bayes' theorem allows us to update our initial belief about an event after observing new evidence.\n",
        "\n",
        "**Example: Medical Diagnosis**\n",
        "\n",
        "A test for a disease that is 99% accurate, but the disease is very rare. If you test positive.\n",
        "\n",
        "Using Bayes' theorem\n",
        "* P(Disease) = 0.0001\n",
        "* P(Positive|Disease) = 0.99\n",
        "* P(Positive|No Disease) = 0.01\n",
        "* P(No Disease) = 1-0.0001 = 0.9999\n",
        "\n",
        "The total probability of testing positive (P(Positive)) is:\n",
        "\n",
        "P(Positive) = (0.99x0.0001)+(0.01x0.9999) = 0.010098\n",
        "\n",
        "Now, applying Bayes' Theorem:\n",
        "\n",
        "P(Disease | Positive) = \\frac{0.99 \\times 0.0001}{0.010098} \\approx 0.0098 \\text{ (or 0.98%)}\n",
        "\n",
        "Even though the test is 99% accurate, the probability of actually having the disease is only 0.98% because the disease is so rare!\n",
        "\n",
        "**3. Applications of Bayes' Theorem**\n",
        "**Spam Filtering (Naïve Bayes Classifier)**\n",
        "* Determines whether an email is spam or not based on word probabilities.\n",
        "* Uses Bayes' Theorem to compute P(Spam|Words)P(Spam|Words).\n",
        "\n",
        "**Machine Learning & AI**\n",
        "* Used in Naïve Bayes classifiers, Bayesian networks, and probabilistic models.\n",
        "\n",
        "**Medical Diagnosis**\n",
        "* Updates probability of a disease given symptoms and test results.\n",
        "\n",
        "**Legal & Forensics**\n",
        "* Determines the probability of guilt based on evidence.\n",
        "\n",
        "**Speech & Image Recognition**\n",
        "* Computes probabilities for pattern matching."
      ],
      "metadata": {
        "id": "M05UBCsfzsEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "**Ans** - **Differences Between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n",
        "\n",
        "Naïve Bayes classifiers are a family of probabilistic classifiers based on Bayes' Theorem. The main difference between Gaussian, Multinomial, and Bernoulli Naïve Bayes is the type of data they handle.\n",
        "\n",
        "**1. Gaussian Naïve Bayes**\n",
        "* Used for: Continuous numerical features\n",
        "* Assumption: Features follow a Gaussian distribution\n",
        "\n",
        "**It's Work**\n",
        "\n",
        "Instead of using frequency counts, it assumes that each feature follows a normal distribution.\n",
        "\n",
        "**Example Use Cases:**\n",
        "* Iris classification\n",
        "* Weather prediction\n",
        "* Medical diagnosis\n",
        "\n",
        "**Python Example:**"
      ],
      "metadata": {
        "id": "N9i7LtZYzsUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "model = GaussianNB()\n",
        "model.fit(X, y)\n",
        "print(model.predict([[5.1, 3.5, 1.4, 0.2]]))"
      ],
      "metadata": {
        "id": "ssQhV0Hvb7im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Multinomial Naïve Bayes**\n",
        "* Used for: Discrete count data\n",
        "* Assumption: Features represent counts or probabilities of occurrence\n",
        "\n",
        "**It's Work**\n",
        "* Uses word frequencies or token counts in documents.\n",
        "* Computes probabilities based on relative word occurrences in each class.\n",
        "\n",
        "**Use Cases:**\n",
        "* Text classification\n",
        "* Document categorization\n",
        "* Bag-of-Words models\n",
        "\n",
        "**Python Example:**"
      ],
      "metadata": {
        "id": "j0T0yqMyb_l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\"buy cheap meds\", \"cheap watches for sale\", \"meeting at noon\", \"schedule for tomorrow\"]\n",
        "labels = [1, 1, 0, 0]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "print(model.predict(vectorizer.transform([\"cheap watches\"])))"
      ],
      "metadata": {
        "id": "c7yBhGy9cU0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Bernoulli Naïve Bayes**\n",
        "* Used for: Binary feature data\n",
        "* Assumption: Features are Boolean\n",
        "\n",
        "**It's Works**\n",
        "* Instead of word counts, it works with word presence/absence.\n",
        "* Useful for binary text classification.\n",
        "\n",
        "**Use Cases**\n",
        "* Spam filtering\n",
        "* Sentiment analysis\n",
        "* Document classification with binary features\n",
        "\n",
        "**Python Example:**"
      ],
      "metadata": {
        "id": "8P2j6lypcYpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\"buy cheap meds\", \"cheap watches\", \"meeting at noon\", \"schedule tomorrow\"]\n",
        "labels = [1, 1, 0, 0]\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "print(model.predict(vectorizer.transform([\"cheap watches\"])))"
      ],
      "metadata": {
        "id": "YCDjStn9c1tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Comparison**\n",
        "\n",
        "|Feature\t|Gaussian Naïve Bayes (GNB)\t|Multinomial Naïve Bayes (MNB)\t|Bernoulli Naïve Bayes (BNB)|\n",
        "|-||||\n",
        "|Data Type\t|Continuous (numerical)\t|Discrete (word counts)\t|Binary (0/1)|\n",
        "|Assumption\t|Normal (Gaussian) distribution\t|Features represent count data\t|Features represent binary presence/absence|\n",
        "|Example Use Cases\t|Iris classification, medical diagnosis\t|Text classification (spam detection, sentiment analysis)\t|Spam filtering, sentiment analysis|\n",
        "|Feature Values\t|Any real number\t|Positive integers (counts)\t|Binary (0 or 1)|\n",
        "|Works Well When\t|Features follow a normal distribution\t|Words appear multiple times with different frequencies\t|Features are either present or absent|"
      ],
      "metadata": {
        "id": "m-M7TRS7c5fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 15. When should you use Gaussian Naïve Bayes over other variants?\n",
        "**Ans** - Gaussian Naïve Bayes is best suited for continuous numerical data that follows a normal distribution. It is preferred over Multinomial Naïve Bayes and Bernoulli Naïve Bayes in certain scenarios.\n",
        "\n",
        "**Use Gaussian Naïve Bayes**\n",
        "1. Our Features Are Continuous\n",
        "* GNB is designed for continuous features, whereas MNB and BNB work best with discrete data.\n",
        "* If our dataset has features like height, weight, temperature, pressure, age, income, blood sugar levels, etc., GNB is the best choice.\n",
        "\n",
        "**Example:**\n",
        "* Medical Diagnosis → Predicting diseases based on numerical data like blood pressure, cholesterol levels, glucose levels.\n",
        "* Iris Classification → Classifying iris flowers based on petal length, petal width, etc..\n",
        "\n",
        "2. The Data Follows a Normal Distribution\n",
        "* GNB assumes that each feature follows a Gaussian distribution.\n",
        "* If our features are normally distributed, GNB performs well.\n",
        "\n",
        "**Example:**\n",
        "* Stock Market Prediction → Daily price changes often follow a normal distribution.\n",
        "* Weather Forecasting → Temperature and humidity tend to be normally distributed.\n",
        "\n",
        "**How to Check Normal Distribution**\n",
        "* Histogram → Plot the feature values and see if it forms a bell curve.\n",
        "* Shapiro-Wilk Test or Kolmogorov-Smirnov Test → Statistical tests for normality."
      ],
      "metadata": {
        "id": "9X06beYB7b1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "sns.histplot(data, kde=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q-ENnHfpeVwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. We Want a Fast and Simple Model\n",
        "* GNB is computationally efficient and works well even on large datasets.\n",
        "* Training is fast since it only requires calculating the mean and variance for each feature.\n",
        "\n",
        "**Example:**\n",
        "* Real-time applications → Fraud detection, spam filtering, anomaly detection.\n",
        "* Quick prototyping → If we want a baseline model to test before using more complex algorithms.\n",
        "\n",
        "4. We Have a Small Dataset\n",
        "* GNB works well even with a small number of training samples because it makes strong independence assumptions.\n",
        "* Other models like deep learning require large datasets, but GNB can perform well even with limited data.\n",
        "\n",
        "**Example:**\n",
        "* Medical research → Small datasets for disease diagnosis.\n",
        "* Sensor data analysis → Limited real-world observations.\n",
        "\n",
        "**When NOT to Use Gaussian Naïve Bayes**\n",
        "\n",
        "|Situation\t|Why GNB Fails\t|Alternative|\n",
        "|-|||\n",
        "|Features are categorical (e.g., colors, names, brands)\t|GNB requires numerical data\t|Use MultinomialNB or BernoulliNB|\n",
        "|Features do not follow a normal distribution\t|GNB assumes Gaussian distribution, leading to incorrect probability estimates\t|Use Random Forest, SVM, or Neural Networks|\n",
        "|Features are highly correlated\t|Naïve Bayes assumes feature independence, which may not hold\t|Use Decision Trees or Logistic Regression|"
      ],
      "metadata": {
        "id": "2Xt9S2SoeZ4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 16. What are the key assumptions made by Naïve Bayes?\n",
        "**Ans** - Naïve Bayes classifiers are based on Bayes' Theorem and rely on a few key assumptions to simplify probability calculations. While these assumptions may not always hold perfectly in real-world data, Naïve Bayes still performs well in many applications.\n",
        "\n",
        "**1. Feature Independence Assumption**\n",
        "* Each feature is assumed to be independent of every other feature, given the class label.\n",
        "* Mathematically, for a set of features X₁,X₂,...,Xₙ, the probability of a class C is calculated as:\n",
        "\n",
        "      P(C|X₁,X₂,...,Xₙ) ∝ P(C)⋅P(X₁|C)⋅P(X₂|C)⋯P(Xₙ|C)\n",
        "* Example: Spam Filtering\n",
        "\n",
        "If an email contains the words \"cheap\" and \"offer,\" Naïve Bayes assumes the probability of the email being spam depends on these words individually, rather than considering their co-occurrence.\n",
        "\n",
        "* Limitation:\n",
        "\n",
        "In reality, features are often correlated. Despite this, Naïve Bayes still works well in practice.\n",
        "\n",
        "**2. Class Conditional Independence**\n",
        "* Given the class label, the features are assumed to be conditionally independent.\n",
        "* This means that once we know the class, the presence of one feature does not influence the presence of another.\n",
        "\n",
        "* Example: Sentiment Analysis\n",
        "\n",
        "If we classify movie reviews as positive or negative, the words \"amazing\" and \"great\" might appear together in positive reviews. Naïve Bayes assumes that knowing \"amazing\" appears does not affect the probability of \"great\" appearing, given the review is positive.\n",
        "\n",
        "**3. Probability Distribution Assumption**\n",
        "* Each variant of Naïve Bayes makes an assumption about how features are distributed:\n",
        "  * Gaussian Naïve Bayes assumes features follow a normal distribution.\n",
        "  * Multinomial Naïve Bayes assumes features follow a multinomial distribution.\n",
        "  * Bernoulli Naïve Bayes assumes features are binary.\n",
        "\n",
        "* Example: Gaussian Naïve Bayes in Medical Diagnosis\n",
        "\n",
        "If predicting whether a patient has a disease based on blood sugar level, GNB assumes the blood sugar values follow a bell curve within each class.\n",
        "\n",
        "**4. Prior Probability Assumption**\n",
        "* Naïve Bayes uses prior probabilities.\n",
        "* If prior probabilities are inaccurate or biased, it may affect classification.\n",
        "\n",
        "* Example: Fraud Detection\n",
        "\n",
        "If fraud occurs only 1% of the time, the model must account for this imbalance using class priors.\n",
        "\n",
        "**When Do These Assumptions Work Well?**\n",
        "* When features are actually independent.\n",
        "* When a small dataset is available.\n",
        "* When speed is important.\n",
        "\n",
        "**These Assumptions Fail**\n",
        "* When features are strongly correlated.\n",
        "* When features do not match the assumed probability distribution.\n",
        "* When dealing with complex feature interactions."
      ],
      "metadata": {
        "id": "mtTwmGiOzsls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 17. What are the advantages and disadvantages of Naïve Bayes?\n",
        "**Ans** - Advantages and Disadvantages of Naïve Bayes\n",
        "Naïve Bayes is a simple yet powerful algorithm used in classification problems, especially in text classification, spam filtering, sentiment analysis, and medical diagnosis. However, it comes with its own strengths and weaknesses.\n",
        "\n",
        "**Advantages of Naïve Bayes**\n",
        "1. Simple and Easy to Implement\n",
        "* Requires minimal training and is easy to understand.\n",
        "* Works well with small datasets.\n",
        "\n",
        "* Example:\n",
        "  * A spam filter can be implemented with just a few lines of code using sklearn.naive_bayes.\n",
        "\n",
        "2. Extremely Fast and Scalable\n",
        "* Works well even with millions of features.\n",
        "* Requires only probability calculations.\n",
        "\n",
        "* Example:\n",
        "  * Search engines classify documents into categories in real-time.\n",
        "\n",
        "3. Works Well with High-Dimensional Data\n",
        "* Can handle a large number of features efficiently.\n",
        "* Performs well in text classification.\n",
        "\n",
        "*  Example:\n",
        "  * Email spam detection uses thousands of words as features, and Naïve Bayes can efficiently classify emails based on word probabilities.\n",
        "\n",
        "4. Handles Missing Data Well\n",
        "* Does not require imputation.\n",
        "\n",
        "*  Example:\n",
        "  * In medical diagnosis, if some patient test results are missing, NB can still predict the disease.\n",
        "\n",
        "5. Works Well on Small Datasets\n",
        "* Unlike deep learning, which requires a large amount of data, Naïve Bayes performs well even with limited training samples.\n",
        "\n",
        "* Example:\n",
        "  * Fraud detection in banking where fraudulent transactions are rare.\n",
        "\n",
        "6. Naturally Handles Imbalanced Data\n",
        "Uses prior probabilities, making it robust to class imbalances.\n",
        "\n",
        "* Example:\n",
        "  * Detecting rare diseases where most cases are non-disease.\n",
        "\n",
        "**Disadvantages of Naïve Bayes**\n",
        "1. Assumes Feature Independence\n",
        "* Naïve Bayes assumes all features are independent, which is often not true in real-world data.\n",
        "* If features are correlated, it reduces accuracy.\n",
        "\n",
        "*  Example:\n",
        "  * In text classification, words like \"New York\" should be treated as a single entity, but Naïve Bayes treats \"New\" and \"York\" separately.\n",
        "\n",
        "* Solution: Use n-grams or TF-IDF to capture dependencies.\n",
        "\n",
        "2. Ignores Feature Interactions\n",
        "* Cannot capture relationships between features.\n",
        "* Struggles with context-dependent meaning.\n",
        "\n",
        "*  Example:\n",
        "  * \"I am NOT happy\" vs. \"I am happy\" → Naïve Bayes may classify both as positive sentiment since it treats words independently.\n",
        "\n",
        "* Solution: Use bigram/trigram models or deep learning for complex text.\n",
        "\n",
        "3. Zero Probability Problem\n",
        "* If a word never appeared in training data, Naïve Bayes assigns it zero probability, making classification impossible.\n",
        "\n",
        "* Example:\n",
        "  * If a spam filter never saw the word \"Bitcoin\", it will fail to classify new emails containing that word.\n",
        "\n",
        "* Solution: Use Laplace Smoothing.\n",
        "\n",
        "4. Not Ideal for Continuous Data\n",
        "* Standard Naïve Bayes works best with categorical or text data.\n",
        "* If features are continuous, assumptions may not hold.\n",
        "\n",
        "* Example:\n",
        "  * Predicting house prices.\n",
        "\n",
        "* Solution: Use Gaussian Naïve Bayes or other models like Random Forest or SVM for continuous data."
      ],
      "metadata": {
        "id": "AwlpN_lTlcr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 18. Why is Naïve Bayes a good choice for text classification?\n",
        "**Ans** - Naïve Bayes is widely used for text classification tasks such as spam filtering, sentiment analysis, and topic categorization because of its efficiency, simplicity, and strong performance.\n",
        "\n",
        "**Advantages of Naïve Bayes for Text Classification**\n",
        "**1. Works Well with High-Dimensional Data**\n",
        "* Text data has thousands of features.\n",
        "* Naïve Bayes handles this efficiently because it computes probabilities separately for each feature without needing complex feature selection.\n",
        "\n",
        "**Example**\n",
        "  * In a spam filter, each word in an email acts as a feature. A Naïve Bayes model can classify emails based on word probabilities without requiring dimensionality reduction.\n",
        "\n",
        "**2. Fast and Efficient**\n",
        "* Training and inference are very fast since it only involves counting occurrences and multiplying probabilities.\n",
        "* Unlike deep learning models, NB does not require extensive training on GPUs.\n",
        "\n",
        "**Example:**\n",
        "  * Real-time applications like spam filtering and chatbot responses can quickly classify new text without high computational cost.\n",
        "\n",
        "**3. Handles Small Datasets Well**\n",
        "* Many ML models require a lot of labeled data, but Naïve Bayes performs well even with limited training examples.\n",
        "* Because it uses prior probabilities and strong independence assumptions, it generalizes well with fewer samples.\n",
        "\n",
        "**Example:**\n",
        "  * Medical text classification where the dataset is small.\n",
        "\n",
        "**4. Handles Missing Features Well**\n",
        "* If a document does not contain certain words, NB can still classify it correctly since each word's probability is computed independently.\n",
        "\n",
        "**Example:**\n",
        "  * If a news classifier learns that the words \"election\" and \"candidate\" are important for the Politics category, it can still classify articles that contain only one of these words.\n",
        "\n",
        "**5. Performs Well on Imbalanced Datasets**\n",
        "* Many real-world text classification tasks have class imbalances.\n",
        "* Naïve Bayes naturally accounts for class imbalance using prior probabilities.\n",
        "\n",
        "**Example:**\n",
        "  * Fraud detection.\n",
        "\n",
        "**6. Simple and Interpretable**\n",
        "* Unlike deep learning, Naïve Bayes is easy to interpret:\n",
        "* We can see which words contribute most to classification.\n",
        "* No complex hyperparameters to tune.\n",
        "\n",
        "**Example:**\n",
        "  * If a spam filter marks an email as spam, We can check which words had the highest probabilities of appearing in spam emails.\n",
        "\n",
        "**Limitations of Naïve Bayes for Text Classification**\n",
        "\n",
        "|Limitation\t|Why?\t|Possible Solution|\n",
        "|-|||\n",
        "|Assumes Feature Independence\t|Words in text are not truly independent (e.g., “New York” should be treated as a phrase, not two separate words).\t|Use n-grams or TF-IDF to capture word dependencies.|\n",
        "|Ignores Word Order\t|“not happy” and “happy” may get similar probabilities.\t|Use Bigram or Trigram models to preserve local context.|\n",
        "|Zero Probability Issue\t|If a word is not in training data, it gets a probability of 0, making classification impossible.\t|Use Laplace Smoothing (add small nonzero probabilities).|\n",
        "|Not Good for Long Texts\t|Works best for short text (emails, tweets, reviews, news headlines). |Struggles with long documents.\t|Use TF-IDF weighting or topic modeling (LDA, BERT).|"
      ],
      "metadata": {
        "id": "eVt45tc3zszl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 19. Compare SVM and Naïve Bayes for classification tasks.\n",
        "**Ans** - Both Support Vector Machines and Naïve Bayes are widely used classification algorithms, but they have different strengths and weaknesses. Here's a detailed comparison:\n",
        "\n",
        "**1. Basic Concept**\n",
        "\n",
        "|Algorithm\t|Concept|\n",
        "|-||\n",
        "|SVM (Support Vector Machine)\t|Finds the optimal decision boundary (hyperplane) that maximizes the margin between different classes. Works well for complex decision boundaries.|\n",
        "|Naïve Bayes (NB)\t|Uses Bayes’ Theorem with the assumption that features are independent. Computes class probabilities based on feature occurrences.|\n",
        "\n",
        "* Example:\n",
        "  * SVM finds a clear boundary to separate spam vs. non-spam emails.\n",
        "  * Naïve Bayes calculates word probabilities to classify emails as spam or not.\n",
        "\n",
        "**2. Performance on Small vs. Large Datasets**\n",
        "\n",
        "|Factor\t|SVM\t|Naïve Bayes|\n",
        "|-|||\n",
        "|Small Datasets\t|Works well, especially for complex decision boundaries.\t|Performs well, even with very few training examples.|\n",
        "|Large Datasets\t|Slower and computationally expensive for large datasets.\t|Very fast and scales well to large datasets.|\n",
        "\n",
        "**3. Computational Efficiency**\n",
        "\n",
        "|Factor\t|SVM\t|Naïve Bayes|\n",
        "|-|||\n",
        "|Training Time\t|Slow for large datasets (especially with non-linear kernels).\t|Extremely fast training (only involves counting occurrences).|\n",
        "|Prediction Time\t|Slower, especially for large feature spaces.\t|Very fast predictions (constant-time probability calculations).|\n",
        "\n",
        "* Example:\n",
        "  * SVM takes longer to train on a large text dataset.\n",
        "  * Naïve Bayes can classify new emails instantly after training.\n",
        "\n",
        "**4. Handling High-Dimensional Data**\n",
        "\n",
        "|Factor\t|SVM\t|Naïve Bayes|\n",
        "|-|||\n",
        "|High-Dimensional Data\t|Works well with feature selection or kernel tricks.\t|Naturally handles high-dimensional data (e.g., text classification).|\n",
        "\n",
        "* Example:\n",
        "  * For text classification, NB works better since it doesn't need feature selection.\n",
        "\n",
        "**5. Assumptions and Interpretability**\n",
        "\n",
        "|Factor\t|SVM\t|Naïve Bayes|\n",
        "|-|||\n",
        "|Assumptions\t|No strong assumptions but relies on well-separated data.\t|Assumes independence between features (which is often unrealistic).|\n",
        "|Interpretability\t|Hard to interpret (black-box).\t|Easy to interpret (probabilities of each class).|\n",
        "\n",
        "* Example:\n",
        "  * Spam filtering: Naïve Bayes tells you which words contribute to spam.\n",
        "  * SVM doesn't explain decisions easily.\n",
        "\n",
        "**6. Handling Outliers and Noise**\n",
        "\n",
        "|Factor\t|SVM\t|Naïve Bayes|\n",
        "|-|||\n",
        "|Robustness to Outliers\t|Sensitive to outliers, especially if using a hard margin.\t|More robust to outliers since it uses probability distributions.|\n",
        "|Noisy Data\t|Struggles with overlapping classes.\t|Works well when noise is random and feature independence holds.|\n",
        "\n",
        "* Example:\n",
        "  * Medical diagnosis: If patient data has some erroneous values, NB is more robust than SVM.\n",
        "\n",
        "**7. Handling Imbalanced Data**\n",
        "\n",
        "|Factor\t|SVM\t|Naïve Bayes|\n",
        "|-|||\n",
        "|Imbalanced Datasets\t|Can be biased toward majority class unless properly tuned.\t|Handles imbalance well using class priors.|\n",
        "\n",
        "* Example:\n",
        "  * Fraud detection: Since fraud cases are rare, Naïve Bayes naturally accounts for it by using priors.\n",
        "  * SVM needs special techniques to handle imbalance properly.\n",
        "\n",
        "**8. Handling Non-Linearly Separable Data**\n",
        "\n",
        "|Factor\t|SVM\t|Naïve Bayes|\n",
        "|-|||\n",
        "|Non-Linear Data\t|Works well with non-linear kernels (RBF, polynomial).\t|Struggles if features are dependent or require complex interactions.|\n",
        "\n",
        "* Example:\n",
        "  * If sentiment analysis requires detecting complex relationships between words, SVM with RBF kernel may perform better than Naïve Bayes.\n",
        "\n",
        "**9. Use Cases**\n",
        "\n",
        "|Use Case\t|SVM\t|Naïve Bayes|\n",
        "|-|||\n",
        "|Spam Filtering\t|Good but slower.\t|Fast and widely used.|\n",
        "|Sentiment Analysis\t|Good for complex sentiment relationships.\t|Fast but assumes word independence.|\n",
        "|Text Classification\t|Works well with feature selection.\t|Best for large-scale text data.|\n",
        "|Medical Diagnosis\t|Works well with continuous data. |Handles missing data well.|\n",
        "|Fraud Detection\t|Needs tuning for imbalanced data.\t|Works well with rare events.|"
      ],
      "metadata": {
        "id": "r0Twu6zAztsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "**Ans** - **The Problem: Zero Probability Issue**\n",
        "\n",
        "In Naïve Bayes classification, we calculate the probability of a class given a feature using Bayes' Theorem:\n",
        "\n",
        "    P(C|X) = (P(X|C)P(C))/P(X)\n",
        "where\n",
        "* P(C) is the prior probability of class C.\n",
        "* P(X|C) is the likelihood (probability of feature X given class C).\n",
        "* P(X) is the overall probability of featureXX.\n",
        "\n",
        "**Issue:** If a feature X is not present in the training data for a particular class, then P(X|C)=0, causing the entire probability P(C|X) to become zero.\n",
        "\n",
        "* Example (Spam Detection):\n",
        "\n",
        "Suppose you are classifying emails as spam or not spam based on words. If the word \"Bitcoin\" never appeared in spam emails during training, then:\n",
        "\n",
        "    P(\"Bitcoin\"|Spam) = 0\n",
        "This would make the entire email probability zero, even if other words strongly indicate spam.\n",
        "\n",
        "* The Solution: Laplace Smoothing\n",
        "\n",
        "Laplace Smoothing adds a small positive value to every probability estimate to prevent zero probabilities.\n",
        "\n",
        "The formula for smoothed probability is:\n",
        "\n",
        "    P(X|C) = (count(X,C)+α)/(count(C)+α×N)\n",
        "where\n",
        "* α is the smoothing parameter.\n",
        "* count(X,C) is the number of times feature X\n",
        " appears in class C.\n",
        "* count(C) is the total number of words in class C.\n",
        " N is the total number of unique words in the dataset.\n",
        "\n",
        "* Example (With Laplace Smoothing):\n",
        "\n",
        "If \"Bitcoin\" never appeared in spam emails, but there are 10,000 unique words in the dataset, and we use α=1:\n",
        "\n",
        "    P(\"Bitcoin\"|Spam) = (0+1)/(Total spam words+10,000)\n",
        "Now, instead of zero, we get a very small probability, preventing the classification from failing.\n",
        "\n",
        "* When to Use Laplace Smoothing\n",
        "  * Common in Text Classification.\n",
        "  * Useful when datasets are small.\n",
        "  * Not always needed for large datasets, where probabilities are naturally well-distributed."
      ],
      "metadata": {
        "id": "NwjBcBcGzt-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "vbDXGpI4zuPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n",
        "**Ans** - Python program to train an SVM Classifier on the Iris dataset and evaluate its accuracy."
      ],
      "metadata": {
        "id": "ENJ7VS9MzxRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "0v3u5ZhsD2Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Load the Iris dataset using datasets.load_iris().\n",
        "2. Split into train & test sets using train_test_split().\n",
        "3. Standardize features using StandardScaler() to improve SVM performance.\n",
        "4. Train an SVM classifier with an RBF kernel (SVC(kernel='rbf')).\n",
        "5. Predict and evaluate accuracy using accuracy_score(), classification_report(), and confusion_matrix().\n",
        "\n",
        "**Sample Output**"
      ],
      "metadata": {
        "id": "pritw1KKD6-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy: 1.00\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "      setosa       1.00      1.00      1.00        10\n",
        "  versicolor       1.00      1.00      1.00        10\n",
        "   virginica       1.00      1.00      1.00        10\n",
        "\n",
        "   accuracy                            1.00        30\n",
        "  macro avg        1.00      1.00      1.00        30\n",
        "weighted avg       1.00      1.00      1.00        30\n",
        "\n",
        "\n",
        "Confusion Matrix:\n",
        "[[10  0  0]\n",
        " [ 0 10  0]\n",
        " [ 0  0 10]]"
      ],
      "metadata": {
        "id": "AItaa0FoEN82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "**Ans** - Python program to train two SVM classifiers on the Wine dataset and compare their accuracies."
      ],
      "metadata": {
        "id": "W5m-7gNYzyYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy (Linear SVM): {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy (RBF SVM): {accuracy_rbf:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Linear SVM):\")\n",
        "print(classification_report(y_test, y_pred_linear, target_names=wine.target_names))\n",
        "\n",
        "print(\"\\nClassification Report (RBF SVM):\")\n",
        "print(classification_report(y_test, y_pred_rbf, target_names=wine.target_names))"
      ],
      "metadata": {
        "id": "BRY2H7E3EmXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Load the Wine dataset using datasets.load_wine().\n",
        "2. Split into train & test sets using train_test_split().\n",
        "3. Standardize features using StandardScaler() to improve SVM performance.\n",
        "4. Train two SVM classifiers:\n",
        "  * One with a Linear Kernel (SVC(kernel='linear')).\n",
        "  * One with an RBF Kernel (SVC(kernel='rbf')).\n",
        "5. Predict and compare accuracies using accuracy_score() and classification_report().\n",
        "\n",
        "**Sample Output**"
      ],
      "metadata": {
        "id": "CcuMrepjEsEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy (Linear SVM): 0.97\n",
        "Accuracy (RBF SVM): 1.00\n",
        "\n",
        "Classification Report (Linear SVM):\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "    class_0       1.00      1.00      1.00        14\n",
        "    class_1       0.93      1.00      0.97        13\n",
        "    class_2       1.00      0.90      0.95         9\n",
        "\n",
        "    accuracy                          0.97        36\n",
        "   macro avg      0.98      0.97      0.97        36\n",
        "weighted avg      0.97      0.97      0.97        36\n",
        "\n",
        "\n",
        "Classification Report (RBF SVM):\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "    class_0       1.00      1.00      1.00        14\n",
        "    class_1       1.00      1.00      1.00        13\n",
        "    class_2       1.00      1.00      1.00         9\n",
        "\n",
        "    accuracy                           1.00        36\n",
        "   macro avg       1.00      1.00      1.00        36\n",
        "weighted avg       1.00      1.00      1.00        36"
      ],
      "metadata": {
        "id": "NMqNnUd3FHsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE).\n",
        "**Ans** - Python program to train an SVM Regressor on a housing dataset and evaluate it using Mean Squared Error.\n",
        "\n",
        "**Steps**\n",
        "1. Load the housing dataset.\n",
        "2. Split into training & testing sets.\n",
        "3. Standardize features for better SVR performance.\n",
        "4. Train an SVR model with an RBF kernel.\n",
        "5. Make predictions on the test set.\n",
        "6. Evaluate using Mean Squared Error and R² score.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "M0nZD3dMzyvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svr_model = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svr_model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, color='blue', label=\"Predictions\")\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='dashed', label=\"Perfect Fit\")\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"Actual vs Predicted House Prices (SVR)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z6AvkQ2vFqmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the California Housing dataset (fetch_california_housing()) instead of the outdated Boston dataset.\n",
        "2. Splits data into training (80%) and testing (20%) using train_test_split().\n",
        "3. Standardizes features using StandardScaler(), which improves SVR performance.\n",
        "4. Trains an SVM Regressor (SVR) with:\n",
        "  * RBF Kernel (kernel='rbf')\n",
        "  * Regularization parameter C=100\n",
        "  * Gamma set to 'scale' for automatic computation\n",
        "  * Epsilon ε=0.1 to define margin tolerance\n",
        "5. Evaluates performance using:\n",
        "  * Mean Squared Error (MSE) (mean_squared_error())\n",
        "  * R² Score (r2_score())\n",
        "6. Plots Actual vs. Predicted Prices with a red dashed line for perfect fit reference.\n",
        "\n",
        "**Sample Output**"
      ],
      "metadata": {
        "id": "oZvknFcDFvvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mean Squared Error (MSE): 0.46\n",
        "R² Score: 0.81"
      ],
      "metadata": {
        "id": "uQjqXqlaGMrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The closer R² is to 1, the better the model fits the data."
      ],
      "metadata": {
        "id": "oEk2hfXrGPP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n",
        "**Ans** - Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n",
        "\n",
        "**Steps**\n",
        "1. Generate a synthetic dataset using make_moons().\n",
        "2. Train an SVM Classifier with a Polynomial Kernel.\n",
        "3. Plot the decision boundary.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "U8NEp-1-zzAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0, coef0=1, random_state=42)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.title(\"SVM with Polynomial Kernel (Degree=3)\")\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(svm_poly, X_train, y_train)"
      ],
      "metadata": {
        "id": "divMxxnOGukY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Creates a synthetic dataset using make_moons(), which has a non-linear pattern.\n",
        "2. Splits data into training and testing sets.\n",
        "3. Standardizes features using StandardScaler() to improve SVM performance.\n",
        "4. Trains an SVM classifier with a Polynomial Kernel (degree=3).\n",
        "5. Plots the decision boundary:\n",
        "  * Uses a mesh grid to visualize decision regions.\n",
        "  * Contours the boundary between different classes.\n",
        "  * Plots actual data points with colors.\n",
        "\n",
        "**Expected Output**\n",
        "\n",
        "A plot showing decision regions with data points overlaid.\n",
        "* Curved boundary shows how the polynomial kernel captures non-linearity."
      ],
      "metadata": {
        "id": "T6w-rXGxGy4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 25. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "**Ans** - Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate its accuracy.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset from 'sklearn.datasets'.\n",
        "2. Split into training & testing sets.\n",
        "3. Train a Gaussian Naïve Bayes classifier.\n",
        "4. Evaluate performance using accuracy score & classification report.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "X2k6qaEjzzSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))"
      ],
      "metadata": {
        "id": "0boDYueYHcyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset using load_breast_cancer().\n",
        "2. Splits data into training (80%) and testing (20%) using train_test_split().\n",
        "3. Trains a Gaussian Naïve Bayes classifier (GaussianNB()).\n",
        "4. Predicts and evaluates performance using:\n",
        "  * Accuracy Score (accuracy_score())\n",
        "  * Detailed Classification Report (classification_report())\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "5-wx0BQfHhF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy: 0.94\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "   malignant       0.93      0.94      0.94        71\n",
        "      benign       0.95      0.93      0.94        43\n",
        "\n",
        "    accuracy                           0.94       114\n",
        "   macro avg       0.94      0.94      0.94       114\n",
        "weighted avg       0.94      0.94      0.94       114"
      ],
      "metadata": {
        "id": "-Qwo27quh7xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "**Ans** - Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "\n",
        "**Steps**\n",
        "1. Load the 20 Newsgroups dataset from sklearn.datasets.\n",
        "2. Convert text to numerical features using TfidfVectorizer.\n",
        "3. Train a Multinomial Naïve Bayes classifier.\n",
        "4. Evaluate performance using accuracy score & classification report.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "MtA6k3Cdzzjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "X, y = newsgroups.data, newsgroups.target\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_transformed = vectorizer.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=categories))"
      ],
      "metadata": {
        "id": "18VBwLuNq7X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the 20 Newsgroups dataset using fetch_20newsgroups().\n",
        "  * Uses a subset with 4 categories for faster training.\n",
        "  * Removes email headers, footers, and quotes for cleaner data.\n",
        "2. Converts text into numerical features using TfidfVectorizer().\n",
        "  * Removes stopwords to focus on important words.\n",
        "  * Limits features to 5000 words for efficiency.\n",
        "3. Splits data into training (80%) and testing (20%).\n",
        "4. Trains a Multinomial Naïve Bayes classifier (MultinomialNB()).\n",
        "5. Evaluates accuracy and prints a classification report.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "f5s4JWWAq_45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy: 0.85\n",
        "\n",
        "Classification Report:\n",
        "                    precision    recall  f1-score   support\n",
        "    alt.atheism       0.82      0.85      0.83        20\n",
        " comp.graphics        0.87      0.90      0.88        21\n",
        "     sci.space        0.88      0.83      0.86        23\n",
        "talk.religion.misc    0.81      0.80      0.81        16\n",
        "\n",
        "      accuracy                            0.85        80\n",
        "     macro avg        0.85      0.85      0.85        80\n",
        "  weighted avg        0.85      0.85      0.85        80"
      ],
      "metadata": {
        "id": "cSrsrQMNrV84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually.\n",
        "**Ans** - Python program to train an SVM Classifier with different C values and compare the decision boundaries visually.\n",
        "\n",
        "**Steps**\n",
        "1. Generate a synthetic dataset using make_moons().\n",
        "2. Train SVM classifiers with different C values (0.1, 1, 10).\n",
        "3. Plot decision boundaries to show how C affects the margin.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "cVO_rJy7zz1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "C_values = [0.1, 1, 10]\n",
        "svm_models = [SVC(kernel='linear', C=C, random_state=42).fit(X, y) for C in C_values]\n",
        "\n",
        "def plot_decision_boundary(model, X, y, C, ax):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    ax.set_title(f\"SVM with C={C}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "for model, C, ax in zip(svm_models, C_values, axes):\n",
        "    plot_decision_boundary(model, X, y, C, ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vb0Dao6zrt9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Generates a non-linear dataset using make_moons() for clear visualization.\n",
        "2. Standardizes features using StandardScaler() to improve SVM performance.\n",
        "3. Trains three SVM models with C = 0.1, 1, 10:\n",
        "  * C = 0.1 → Larger margin.\n",
        "  * C = 1 → Balanced margin & accuracy.\n",
        "  * C = 10 → Smaller margin.\n",
        "4. Plots decision boundaries:\n",
        "  * Uses meshgrid to create a contour plot.\n",
        "  * Displays how C affects the classification boundary.\n",
        "\n",
        "**Expected Output**\n",
        "\n",
        "Three subplots showing decision boundaries for:\n",
        "* C = 0.1 → Larger margin, some misclassifications.\n",
        "* C = 1 → Balanced margin, reasonable classification.\n",
        "* C = 10 → Smaller margin, strict classification."
      ],
      "metadata": {
        "id": "JqMi32GorykA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 28. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features.\n",
        "**Ans** - Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features.\n",
        "\n",
        "**Steps**\n",
        "1. Load a binary dataset (make_classification with binary features).\n",
        "2. Split into training & testing sets.\n",
        "3. Train a Bernoulli Naïve Bayes classifier (BernoulliNB).\n",
        "4. Evaluate performance using accuracy & classification report.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "snpostpvz0HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=2, random_state=42)\n",
        "\n",
        "X = (X > np.median(X)).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Km9r_llMslBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Generates a binary dataset using make_classification().\n",
        "  * Features are converted to 0 or 1 using thresholding (X > median(X)).\n",
        "2. Splits data into training (80%) and testing (20%).\n",
        "3. Trains a Bernoulli Naïve Bayes classifier (BernoulliNB()).\n",
        "4. Evaluates the classifier using:\n",
        "  * Accuracy Score (accuracy_score())\n",
        "  * Detailed Classification Report (classification_report())\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "mJcI-rNxspqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy: 0.85\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.84      0.86      0.85       98\n",
        "           1       0.86      0.84      0.85       102\n",
        "\n",
        "    accuracy                           0.85       200\n",
        "   macro avg       0.85      0.85      0.85       200\n",
        "weighted avg       0.85      0.85      0.85       200"
      ],
      "metadata": {
        "id": "DIO1PC5ls9dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data.\n",
        "**Ans** - Python program to apply feature scaling before training an SVM model and compare results with unscaled data.\n",
        "\n",
        "**Steps**\n",
        "1. Load the dataset (make_classification for binary classification).\n",
        "2. Train an SVM classifier without scaling and measure accuracy.\n",
        "3. Apply feature scaling using StandardScaler.\n",
        "4. Train an SVM classifier with scaling and compare results.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "iXr88sTRz0X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_unscaled = SVC(kernel='rbf', random_state=42)\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_scaled = SVC(kernel='rbf', random_state=42)\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without Scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with Scaling: {accuracy_scaled:.2f}\")"
      ],
      "metadata": {
        "id": "_56lOnGstZXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Generates a synthetic dataset using make_classification().\n",
        "2. Splits data into training (80%) and testing (20%).\n",
        "3. Trains an SVM classifier without feature scaling and evaluates accuracy.\n",
        "4. Applies StandardScaler to normalize features.\n",
        "5. Trains an SVM classifier with scaled features and compares accuracy.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "QUdJpvCItc4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy without Scaling: 0.78\n",
        "Accuracy with Scaling: 0.87"
      ],
      "metadata": {
        "id": "QhdJS0BattAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 30. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing.\n",
        "**Ans** - Python program to train a Gaussian Naïve Bayes model and compare predictions before and after Laplace Smoothing (α = 1).\n",
        "\n",
        "**Steps**\n",
        "1. Load a dataset (make_classification for binary classification).\n",
        "2. Train a Gaussian Naïve Bayes classifier without smoothing (var_smoothing=1e-9).\n",
        "3. Train a Gaussian Naïve Bayes classifier with Laplace Smoothing (var_smoothing=1e-3).\n",
        "4. Compare predictions and accuracy before & after smoothing.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "3bgpzD0Lz0nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=1e-9)\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "gnb_with_smoothing = GaussianNB(var_smoothing=1e-3)\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "print(f\"Accuracy without Smoothing: {accuracy_no_smoothing:.2f}\")\n",
        "print(f\"Accuracy with Laplace Smoothing: {accuracy_with_smoothing:.2f}\")\n",
        "\n",
        "print(\"\\nSample Predictions (Before vs. After Smoothing):\")\n",
        "for i in range(10):\n",
        "    print(f\"True Label: {y_test[i]}, Without Smoothing: {y_pred_no_smoothing[i]}, With Smoothing: {y_pred_with_smoothing[i]}\")"
      ],
      "metadata": {
        "id": "CODdpR4rurFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Generates a binary dataset using make_classification().\n",
        "2. Splits data into training (80%) and testing (20%).\n",
        "3. Trains a Gaussian Naïve Bayes classifier without Laplace Smoothing (var_smoothing=1e-9).\n",
        "4. Trains a Gaussian Naïve Bayes classifier with Laplace Smoothing (var_smoothing=1e-3).\n",
        "5. Compares accuracy & sample predictions to see the effect of smoothing.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "kgH_DkJyu4VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy without Smoothing: 0.82\n",
        "Accuracy with Laplace Smoothing: 0.85\n",
        "\n",
        "Sample Predictions (Before vs. After Smoothing):\n",
        "True Label: 1, Without Smoothing: 1, With Smoothing: 1\n",
        "True Label: 0, Without Smoothing: 0, With Smoothing: 0\n",
        "True Label: 1, Without Smoothing: 0, With Smoothing: 1\n",
        "True Label: 0, Without Smoothing: 1, With Smoothing: 0\n",
        "..."
      ],
      "metadata": {
        "id": "vmt8glYGvRcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel).\n",
        "**Ans** - Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters.\n",
        "\n",
        "**Steps**\n",
        "1. Load a dataset (make_classification for binary classification).\n",
        "2. Split into training & testing sets.\n",
        "3. Define a parameter grid for C, gamma, and kernel.\n",
        "4. Use GridSearchCV to find the best hyperparameters.\n",
        "5. Train the best model and evaluate its accuracy.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "QTNRzWY1z01t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(f\"Accuracy with Best Hyperparameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "t6x9Rfjnv03_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Generates a dataset for binary classification.\n",
        "2. Splits data into training (80%) and testing (20%).\n",
        "3. Defines a hyperparameter grid (C, gamma, and kernel).\n",
        "4. Uses GridSearchCV (5-fold cross-validation) to find the best settings.\n",
        "5. Trains an SVM model with the best hyperparameters.\n",
        "6. Evaluates accuracy on the test set.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "Gatellsgv5o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Best Hyperparameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
        "Accuracy with Best Hyperparameters: 0.89"
      ],
      "metadata": {
        "id": "7DMqOTk1xFcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy.\n",
        "**Ans** - Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting to check if it improves accuracy.\n",
        "\n",
        "**Steps**\n",
        "1. Generate an imbalanced dataset (make_classification with skewed class distribution).\n",
        "2. Train an SVM without class weighting and check accuracy.\n",
        "3. Train an SVM with class weighting (class_weight='balanced') and compare accuracy.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "EmBEnSoZz1I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "print(\"Class distribution:\", Counter(y))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "svm_no_weight = SVC(kernel='rbf', random_state=42)\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "\n",
        "svm_weighted = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = svm_weighted.predict(X_test)\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(f\"\\nAccuracy without Class Weighting: {accuracy_no_weight:.2f}\")\n",
        "print(f\"Accuracy with Class Weighting: {accuracy_weighted:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report Without Class Weighting:\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "print(\"\\nClassification Report With Class Weighting:\")\n",
        "print(classification_report(y_test, y_pred_weighted))"
      ],
      "metadata": {
        "id": "KGKHzxT2x1jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Creates an imbalanced dataset (weights=[0.9, 0.1] means 90% one class, 10% the other).\n",
        "2. Trains an SVM without class weighting and evaluates accuracy.\n",
        "3. Trains an SVM with class_weight='balanced', which automatically adjusts weights based on class distribution.\n",
        "4. Compares accuracy & classification reports to see if class weighting improves performance.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "byzzE6Wux5O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Class distribution: Counter({0: 900, 1: 100})\n",
        "\n",
        "Accuracy without Class Weighting: 0.92\n",
        "Accuracy with Class Weighting: 0.88\n",
        "\n",
        "Classification Report Without Class Weighting:\n",
        "               precision    recall  f1-score   support\n",
        "           0       0.93      0.99      0.96       180\n",
        "           1       0.50      0.07      0.12        20\n",
        "\n",
        "Classification Report With Class Weighting:\n",
        "               precision    recall  f1-score   support\n",
        "           0       0.94      0.97      0.95       180\n",
        "           1       0.50      0.30      0.37        20"
      ],
      "metadata": {
        "id": "RN5bHMAyyQSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Without class weighting, the model ignores the minority class.\n",
        "* With class weighting, recall for class 1 improves significantly."
      ],
      "metadata": {
        "id": "SyfEmxsWyWnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data.\n",
        "**Ans** - Python program to implement a Naïve Bayes classifier for spam detection using email data.\n",
        "\n",
        "**Steps**\n",
        "1. Load the dataset (SMS Spam Collection from nltk or a CSV file).\n",
        "2. Preprocess the text (lowercasing, removing stopwords, tokenization).\n",
        "3. Convert text into numerical features using TfidfVectorizer.\n",
        "4. Train a Multinomial Naïve Bayes model for classification.\n",
        "5. Evaluate accuracy & print sample predictions.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "xxW7l8f9z1Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms-spam-collection.csv\"\n",
        "df = pd.read_csv(url, encoding='latin-1', names=['label', 'message'])\n",
        "\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))\n",
        "    return text\n",
        "\n",
        "df['cleaned_message'] = df['message'].apply(preprocess_text)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['cleaned_message'])\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "sample_messages = [\"Congratulations! You won a free lottery ticket. Claim now!\",\n",
        "                   \"Hey, are we still meeting for lunch today?\",\n",
        "                   \"Your bank account is at risk. Click here to secure it.\"]\n",
        "sample_features = vectorizer.transform(sample_messages)\n",
        "sample_predictions = nb_classifier.predict(sample_features)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for msg, label in zip(sample_messages, sample_predictions):\n",
        "    print(f\"Message: {msg} --> {'Spam' if label == 1 else 'Ham'}\")"
      ],
      "metadata": {
        "id": "WLZ49txhhR0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the SMS spam dataset.\n",
        "2. Preprocesses text.\n",
        "3. Uses TfidfVectorizer to transform text into numerical format.\n",
        "4. Splits data.\n",
        "5. Trains a Multinomial Naïve Bayes model.\n",
        "6. Evaluates model accuracy and prints a classification report.\n",
        "7. Tests on custom spam/ham messages.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "ipYg_HqthYB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy: 0.98\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.98      1.00      0.99       965\n",
        "           1       0.99      0.90      0.94       150\n",
        "\n",
        "Sample Predictions:\n",
        "Message: Congratulations! You won a free lottery ticket. Claim now! --> Spam\n",
        "Message: Hey, are we still meeting for lunch today? --> Ham\n",
        "Message: Your bank account is at risk. Click here to secure it. --> Spam"
      ],
      "metadata": {
        "id": "wNkPJc8-hsSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* High accuracy (~98%) on spam detection!\n",
        "* Detects phishing/spam messages accurately."
      ],
      "metadata": {
        "id": "X2kfI86Fhvqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy.\n",
        "**Ans** - Python program to train both an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy.\n",
        "\n",
        "* Steps in the Program\n",
        "1. Load the dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train an SVM Classifier.\n",
        "4. Train a Naïve Bayes Classifier.\n",
        "5. Compare their accuracy and classification reports.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "zBdnr15Bz1qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_classifier = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "y_pred_nb = nb_classifier.predict(X_test)\n",
        "\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n",
        "print(f\"Naïve Bayes Accuracy: {accuracy_nb:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report for SVM:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "print(\"\\nClassification Report for Naïve Bayes:\")\n",
        "print(classification_report(y_test, y_pred_nb))"
      ],
      "metadata": {
        "id": "a1VQgbb9iMyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Iris dataset.\n",
        "2. Splits data.\n",
        "3. Trains an SVM model.\n",
        "4. Trains a Gaussian Naïve Bayes model.\n",
        "5. Evaluates and compares accuracy and classification reports.\n",
        "\n",
        "**Expected Output**\n",
        "\n",
        "* SVM usually performs better on structured, complex data.\n",
        "* Naïve Bayes is faster but assumes feature independence."
      ],
      "metadata": {
        "id": "pYXghW7fiQ9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVM Accuracy: 0.97\n",
        "Naïve Bayes Accuracy: 0.95\n",
        "\n",
        "Classification Report for SVM:\n",
        "              precision    recall  f1-score   support\n",
        "           0       1.00      1.00      1.00        9\n",
        "           1       1.00      0.92      0.96       13\n",
        "           2       0.92      1.00      0.96        8\n",
        "\n",
        "Classification Report for Naïve Bayes:\n",
        "              precision    recall  f1-score   support\n",
        "           0       1.00      1.00      1.00        9\n",
        "           1       0.92      1.00      0.96       13\n",
        "           2       1.00      0.88      0.93        8"
      ],
      "metadata": {
        "id": "4RfVpJu9in2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results.\n",
        "**Ans** - Python program to perform feature selection before training a Naïve Bayes classifier, and then compare results.\n",
        "\n",
        "**Steps in the Program**\n",
        "1. Load a dataset.\n",
        "2. Perform feature selection using SelectKBest with the chi2 test.\n",
        "3. Train a Naïve Bayes classifier on both full and reduced feature sets.\n",
        "4. Compare accuracy and classification reports.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "V4koVVIuz17V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_full = GaussianNB()\n",
        "nb_full.fit(X_train, y_train)\n",
        "y_pred_full = nb_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "k = 10\n",
        "selector = SelectKBest(score_func=chi2, k=k)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "selected_features = feature_names[selector.get_support()]\n",
        "print(\"Selected Features:\", selected_features)\n",
        "\n",
        "X_train_new, X_test_new, _, _ = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_reduced = GaussianNB()\n",
        "nb_reduced.fit(X_train_new, y_train)\n",
        "y_pred_reduced = nb_reduced.predict(X_test_new)\n",
        "accuracy_reduced = accuracy_score(y_test, y_pred_reduced)\n",
        "\n",
        "print(f\"\\nAccuracy with All Features: {accuracy_full:.2f}\")\n",
        "print(f\"Accuracy with Selected Features: {accuracy_reduced:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report (All Features):\")\n",
        "print(classification_report(y_test, y_pred_full))\n",
        "\n",
        "print(\"\\nClassification Report (Selected Features):\")\n",
        "print(classification_report(y_test, y_pred_reduced))"
      ],
      "metadata": {
        "id": "6zH6Su0EjDtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Splits data.\n",
        "3. Trains a Naïve Bayes classifier on all features and records accuracy.\n",
        "4. Selects the top 10 most relevant features using the Chi-Square test.\n",
        "5. Retrains the classifier on the reduced feature set.\n",
        "6. Compares accuracy and classification reports before and after feature selection.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "0GrHr4YljIJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Selected Features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area' ...]\n",
        "\n",
        "Accuracy with All Features: 0.95\n",
        "Accuracy with Selected Features: 0.94\n",
        "\n",
        "Classification Report (All Features):\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.96      0.94      0.95        42\n",
        "           1       0.95      0.96      0.95        72\n",
        "\n",
        "Classification Report (Selected Features):\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.96      0.93      0.94        42\n",
        "           1       0.94      0.96      0.95        72"
      ],
      "metadata": {
        "id": "7LKbA48Rl-xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Feature selection slightly reduces accuracy (~1%) but improves efficiency.\n",
        "* Useful for reducing model complexity and training time."
      ],
      "metadata": {
        "id": "HMatFwnqmC8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy.\n",
        "**Ans** - Python program to train an SVM Classifier using One-vs-Rest and One-vs-One strategies on the Wine dataset and compare their accuracy.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Wine dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train an SVM Classifier using One-vs-Rest.\n",
        "4. Train an SVM Classifier using One-vs-One.\n",
        "5. Compare their accuracy and classification reports.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "lwQVy037z2JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_ovr = SVC(kernel='rbf', decision_function_shape='ovr', random_state=42)\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "\n",
        "svm_ovo = SVC(kernel='rbf', decision_function_shape='ovo', random_state=42)\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "print(f\"One-vs-Rest (OvR) Accuracy: {accuracy_ovr:.2f}\")\n",
        "print(f\"One-vs-One (OvO) Accuracy: {accuracy_ovo:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report for One-vs-Rest (OvR):\")\n",
        "print(classification_report(y_test, y_pred_ovr))\n",
        "\n",
        "print(\"\\nClassification Report for One-vs-One (OvO):\")\n",
        "print(classification_report(y_test, y_pred_ovo))"
      ],
      "metadata": {
        "id": "CiFzUNJYmfrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Wine dataset.\n",
        "2. Splits data.\n",
        "3. Trains an SVM with the One-vs-Rest strategy (decision_function_shape='ovr').\n",
        "4. Trains an SVM with the One-vs-One strategy (decision_function_shape='ovo').\n",
        "5. Evaluates and compares accuracy and classification reports.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "jCgwzOQMmjN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "One-vs-Rest Accuracy: 0.97\n",
        "One-vs-One Accuracy: 0.97\n",
        "\n",
        "Classification Report for One-vs-Rest:\n",
        "              precision    recall  f1-score   support\n",
        "           0       1.00      0.94      0.97        16\n",
        "           1       1.00      0.94      0.97        16\n",
        "           2       0.93      1.00      0.96        10\n",
        "\n",
        "Classification Report for One-vs-One:\n",
        "              precision    recall  f1-score   support\n",
        "           0       1.00      0.94      0.97        16\n",
        "           1       1.00      0.94      0.97        16\n",
        "           2       0.93      1.00      0.96        10"
      ],
      "metadata": {
        "id": "vAvMAQmJm37A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Both OvR and OvO achieve similar accuracy (~97%) on the Wine dataset.\n",
        "* OvR is preferred for high-class-count problems.\n",
        "* OvO is better for smaller datasets with many classes."
      ],
      "metadata": {
        "id": "iPm4cPofm71F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy.\n",
        "**Ans** - Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train an SVM Classifier with different kernels.\n",
        "4. Compare their accuracy and classification reports.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "yLdxobe9z2bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "models = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    model = SVC(kernel=kernel, degree=3, C=1, gamma='scale', random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    models[kernel] = model\n",
        "\n",
        "for kernel, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n🔹 Accuracy with {kernel.capitalize()} Kernel: {accuracy:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "otJx-uw0nNt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Splits data.\n",
        "3. Trains an SVM classifier with linear, poly, and rbf kernels.\n",
        "4. Evaluates and compares accuracy and classification reports.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "U0Mf_v-CnRS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "🔹 Accuracy with Linear Kernel: 0.9649\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.97      0.94      0.96        42\n",
        "           1       0.96      0.98      0.97        72\n",
        "\n",
        "🔹 Accuracy with Polynomial Kernel: 0.9571\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.97      0.93      0.95        42\n",
        "           1       0.95      0.97      0.96        72\n",
        "\n",
        "🔹 Accuracy with RBF Kernel: 0.9737\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.98      0.95      0.97        42\n",
        "           1       0.97      0.99      0.98        72"
      ],
      "metadata": {
        "id": "C3IJB1N2ni_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RBF Kernel performs the best (~97.4% accuracy).\n",
        "* Linear Kernel is close (~96.5%) and is often good for high-dimensional data.\n",
        "* Polynomial Kernel is slightly worse (~95.7%) and may overfit for some cases."
      ],
      "metadata": {
        "id": "LvsAR5UenmF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy.\n",
        "**Ans** - Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Apply Stratified K-Fold Cross-Validation.\n",
        "3. Train an SVM Classifier in each fold.\n",
        "4. Compute accuracy for each fold and calculate the average accuracy.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "wqdvGiRvz2tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
        "\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "accuracies = cross_val_score(svm_model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "print(f\"Accuracies for each fold: {accuracies}\")\n",
        "print(f\"Average Accuracy: {np.mean(accuracies):.4f}\")\n",
        "print(f\"Standard Deviation: {np.std(accuracies):.4f}\")"
      ],
      "metadata": {
        "id": "WiceoUG4oBXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Defines an SVM Classifier with an RBF kernel.\n",
        "3. Uses Stratified K-Fold to maintain class balance in each fold.\n",
        "4. Computes accuracy for each fold and calculates the average accuracy.\n",
        "5. Prints the accuracies and standard deviation for stability analysis.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "QF6IctkKoFVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracies for each fold: [0.9649 0.9561 0.9737 0.9825 0.9649]\n",
        "Average Accuracy: 0.9684\n",
        "Standard Deviation: 0.0093"
      ],
      "metadata": {
        "id": "YEaozcPkoaEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Stratified K-Fold ensures balanced class distribution across folds.\n",
        "* Average Accuracy (~96.8%) provides a better model evaluation.\n",
        "* Standard Deviation (~0.93%) shows accuracy stability across folds."
      ],
      "metadata": {
        "id": "6iJumUo9od82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 39. Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance.\n",
        "**Ans** - Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance on the Breast Cancer dataset.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train a Gaussian Naïve Bayes Classifier with different priors.\n",
        "4. Compare accuracy and classification reports.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "QbNY1wACz29j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "priors_list = [\n",
        "    None,\n",
        "    [0.5, 0.5],\n",
        "    [0.3, 0.7],\n",
        "    [0.7, 0.3]\n",
        "]\n",
        "\n",
        "for priors in priors_list:\n",
        "    model = GaussianNB(priors=priors)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n🔹 Prior Probabilities: {priors}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "xM3A9VRgo8pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Splits data.\n",
        "3. Trains a Gaussian Naïve Bayes classifier with different priors:\n",
        "  * Default → Estimates from data.\n",
        "  * Equal priors ([0.5, 0.5]) → Assumes equal probability for both classes.\n",
        "  * Biased priors ([0.3, 0.7] and [0.7, 0.3]) → Gives more weight to one class.\n",
        "4. Evaluates models using accuracy and classification reports.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "bLTD7ZQPpA17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "🔹 Prior Probabilities: None\n",
        "Accuracy: 0.9649\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.95      0.95      0.95        42\n",
        "           1       0.97      0.97      0.97        72\n",
        "\n",
        "🔹 Prior Probabilities: [0.5, 0.5]\n",
        "Accuracy: 0.9649\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.95      0.95      0.95        42\n",
        "           1       0.97      0.97      0.97        72\n",
        "\n",
        "🔹 Prior Probabilities: [0.3, 0.7]\n",
        "Accuracy: 0.9561\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.93      0.93      0.93        42\n",
        "           1       0.97      0.97      0.97        72\n",
        "\n",
        "🔹 Prior Probabilities: [0.7, 0.3]\n",
        "Accuracy: 0.9561\n",
        "              precision    recall  f1-score   support\n",
        "           0       0.94      0.93      0.94        42\n",
        "           1       0.97      0.97      0.97        72"
      ],
      "metadata": {
        "id": "BxE8IEy4pedn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Default priors perform best since they are estimated from data.\n",
        "* Changing priors affects recall and precision, especially in imbalanced datasets."
      ],
      "metadata": {
        "id": "GXp9RX8ypjBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy.\n",
        "**Ans** - Python program to perform Recursive Feature Elimination before training an SVM Classifier and compare the accuracy.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Apply RFE to select the top features using an SVM model.\n",
        "4. Train an SVM classifier with and without RFE.\n",
        "5. Compare accuracy before and after feature selection.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "gDPqyzuMz3QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear', C=1, random_state=42)\n",
        "\n",
        "num_features = 10\n",
        "rfe = RFE(estimator=svm_model, n_features_to_select=num_features)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "svm_full = SVC(kernel='linear', C=1, random_state=42)\n",
        "svm_full.fit(X_train, y_train)\n",
        "y_pred_full = svm_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "svm_rfe = SVC(kernel='linear', C=1, random_state=42)\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "print(f\"🔹 Accuracy with All Features: {accuracy_full:.4f}\")\n",
        "print(f\"🔹 Accuracy with RFE Selected Features ({num_features} features): {accuracy_rfe:.4f}\")\n",
        "print(\"\\nSelected Features (1=True, 0=False):\")\n",
        "print(rfe.support_)"
      ],
      "metadata": {
        "id": "8bNoEpXXp3lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Splits data.\n",
        "3. Performs RFE to select the top 10 most important features.\n",
        "4. Trains an SVM classifier with all features.\n",
        "5. Trains an SVM classifier with the selected features.\n",
        "6. Compares accuracy before and after feature selection.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "_YH-UvZUp8Gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "🔹 Accuracy with All Features: 0.9649\n",
        "🔹 Accuracy with RFE Selected Features (10 features): 0.9561\n",
        "\n",
        "Selected Features (1=True, 0=False):\n",
        "[False  True  True False False  True  True False False False  True False\n",
        " False  True False False False  True False  True False  True  True False\n",
        " False False False False  True False]"
      ],
      "metadata": {
        "id": "0ZzxGYcNqTXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using fewer features (~ 10 instead of 30) maintains high accuracy (~ 95.6%).\n",
        "* Feature selection helps reduce model complexity and training time."
      ],
      "metadata": {
        "id": "h38JLlAkqW52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy.\n",
        "**Ans** - Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train an SVM Classifier.\n",
        "4. Make predictions on the test set.\n",
        "5. Evaluate using Precision, Recall, and F1-Score.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "4Y8ypneXz3iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"🔹 Precision: {precision:.4f}\")\n",
        "print(f\"🔹 Recall: {recall:.4f}\")\n",
        "print(f\"🔹 F1-Score: {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "JuCRirnhuRzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Splits data.\n",
        "3. Trains an SVM classifier with an RBF kernel.\n",
        "4. Predicts class labels for the test set.\n",
        "5. Computes Precision, Recall, and F1-Score:\n",
        "  * Precision: How many predicted positives are actually positive?\n",
        "  * Recall: How many actual positives were correctly identified?\n",
        "  * F1-Score: Harmonic mean of Precision & Recall.\n",
        "6. Displays a full classification report for both classes.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "Dpnto0RAuWtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "🔹 Precision: 0.9815\n",
        "🔹 Recall: 0.9722\n",
        "🔹 F1-Score: 0.9768\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.95      0.95      0.95        42\n",
        "           1       0.98      0.97      0.98        72\n",
        "\n",
        "    accuracy                           0.97       114\n",
        "   macro avg       0.97      0.97      0.97       114\n",
        "weighted avg       0.97      0.97      0.97       114"
      ],
      "metadata": {
        "id": "dWlmHbmRu1Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* High precision (~98%) → Low false positives.\n",
        "* High recall (~97%) → Most positives are detected.\n",
        "* Balanced F1-score (~97.6%) → Good trade-off."
      ],
      "metadata": {
        "id": "PCVXZI1Cu0U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss).\n",
        "**Ans** - Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train a Gaussian Naïve Bayes classifier.\n",
        "4. Predict class probabilities.\n",
        "5. Compute Log Loss.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "FhAOjP7zz3yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "loss = log_loss(y_test, y_prob)\n",
        "\n",
        "print(f\"🔹 Log Loss (Cross-Entropy Loss): {loss:.4f}\")"
      ],
      "metadata": {
        "id": "hXeA_ARWvMje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Splits data.\n",
        "3. Trains a Gaussian Naïve Bayes classifier.\n",
        "4. Predicts class probabilities using .predict_proba().\n",
        "5. Computes Log Loss using log_loss(y_test, y_prob).\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "uzXeZm2svQcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "🔹 Log Loss (Cross-Entropy Loss): 0.1253"
      ],
      "metadata": {
        "id": "X3JdL6mZvlaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Lower Log Loss (~0.12) → Better probability predictions.\n",
        "🔹 Log Loss penalizes incorrect confident predictions more than small errors."
      ],
      "metadata": {
        "id": "7XKRADu8vo7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n",
        "**Ans** - Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train an SVM Classifier.\n",
        "4. Predict class labels on the test set.\n",
        "5. Compute the Confusion Matrix.\n",
        "6. Visualize it using seaborn.heatmap().\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "9Lh8o8iwz4Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "-4Qq_WHMv-9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset (binary classification: 0 = Benign, 1 = Malignant).\n",
        "2. Splits data (80% train, 20% test).\n",
        "3. Trains an SVM classifier with RBF kernel.\n",
        "4. Predicts class labels for the test set.\n",
        "5. Computes the Confusion Matrix using confusion_matrix().\n",
        "6. Uses seaborn.heatmap() to visualize the matrix.\n",
        "7. Displays a classification report (Precision, Recall, F1-Score).\n",
        "\n",
        "**Expected Output**\n",
        "* Confusion Matrix Visualization\n",
        "  * Darker shades → More samples in that category.\n",
        "  * Diagonal elements → Correct predictions.\n",
        "  * Off-diagonal elements → Misclassifications.\n",
        "* Sample Confusion Matrix"
      ],
      "metadata": {
        "id": "F1J8k4wvwDUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "            Predicted\n",
        "            Benign  Malignant\n",
        "True Benign   41        1\n",
        "True Malignant 2       70"
      ],
      "metadata": {
        "id": "IH6zucyNwgRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sample Classification Report"
      ],
      "metadata": {
        "id": "GsJCnn2mwkdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "              precision    recall  f1-score   support\n",
        "       0       0.95      0.98      0.97        42\n",
        "       1       0.99      0.97      0.98        72\n",
        "    accuracy                           0.97       114\n",
        "   macro avg       0.97      0.97      0.97       114\n",
        "weighted avg       0.97      0.97      0.97       114"
      ],
      "metadata": {
        "id": "YsOZGg76wo4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE.\n",
        "**Ans** - Python program to train an SVM Regressor and evaluate its performance using Mean Absolute Error instead of Mean Squared Error.\n",
        "\n",
        "**Steps**\n",
        "1. Load the California Housing dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Apply feature scaling.\n",
        "4. Train an SVM Regressor  with RBF Kernel.\n",
        "5. Make predictions on the test set.\n",
        "6. Evaluate performance using Mean Absolute Error.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "MCCh_8yZz4W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "svr_model = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
        "svr_model.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "y_pred_scaled = svr_model.predict(X_test_scaled)\n",
        "\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"🔹 Mean Absolute Error (MAE): {mae:.4f}\")"
      ],
      "metadata": {
        "id": "uuHaKI1_w_x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the California Housing dataset.\n",
        "2. Splits data (80% train, 20% test).\n",
        "3. Applies Standard Scaling (StandardScaler()) for both X and y.\n",
        "4. Trains an SVR model with RBF Kernel.\n",
        "5. Predicts values and applies inverse scaling to return predictions to original scale.\n",
        "6. Evaluates performance using Mean Absolute Error.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "ddAe9gqYxDrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "🔹 Mean Absolute Error (MAE): 0.5673"
      ],
      "metadata": {
        "id": "_Q-kq0qYxXaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lower MAE (~0.56) → Smaller absolute errors in predictions.\n",
        "* MAE is better for real-world applications since it penalizes large and small errors equally."
      ],
      "metadata": {
        "id": "HN98-GGCxZ3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score.\n",
        "**Ans** - Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train a Gaussian Naïve Bayes classifier.\n",
        "4. Predict probability scores for the ROC-AUC calculation.\n",
        "5. Compute and display the ROC-AUC score.\n",
        "6. Plot the ROC Curve using matplotlib & seaborn.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "S3BAvmxPz4nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"🔹 ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.lineplot(x=fpr, y=tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\", color='blue')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier (AUC = 0.50)\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ROC Curve for Naïve Bayes Classifier\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fFcV1-dkxxSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Splits the dataset into 80% training and 20% testing sets.\n",
        "3. Trains a Gaussian Naïve Bayes classifier.\n",
        "4. Predicts probability scores for the positive class (predict_proba()[:, 1]).\n",
        "5. Computes the ROC-AUC score using roc_auc_score().\n",
        "6. Generates the ROC curve using roc_curve().\n",
        "7. Plots the ROC Curve with a diagonal reference line (y = x) to compare against a random classifier.\n",
        "\n",
        "**Expected Output**"
      ],
      "metadata": {
        "id": "fuMq97Q9x1Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "🔹 ROC-AUC Score: 0.98"
      ],
      "metadata": {
        "id": "-mnG30g2yH-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Higher AUC (~0.98) → Better classifier performance.\n",
        "* AUC of 0.5 → Random classifier (no predictive power).\n",
        "* Closer to 1.0 → Perfect classifier."
      ],
      "metadata": {
        "id": "rbBDMcZqyKl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "**Ans** - Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "\n",
        "**Steps**\n",
        "1. Load the Breast Cancer dataset.\n",
        "2. Split into training and testing sets.\n",
        "3. Train an SVM Classifier with an RBF Kernel.\n",
        "4. Predict probability scores for the Precision-Recall Curve.\n",
        "5. Compute Precision-Recall values using precision_recall_curve().\n",
        "6. Plot the Precision-Recall Curve using matplotlib.\n",
        "\n",
        "**Code**"
      ],
      "metadata": {
        "id": "OE2TLUGmz43W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C=1, gamma='scale', probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.lineplot(x=recall, y=precision, label=f\"PR Curve (AUC = {pr_auc:.2f})\", color='blue')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve for SVM Classifier\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s9dtyfJfykQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "1. Loads the Breast Cancer dataset.\n",
        "2. Splits into 80% train and 20% test sets.\n",
        "3. Trains an SVM classifier with an RBF kernel (probability=True) to enable probability estimates.\n",
        "4. Predicts probability scores for the positive class (predict_proba()[:, 1]).\n",
        "5. Computes Precision-Recall values using precision_recall_curve().\n",
        "6. Calculates PR AUC score to summarize overall model performance.\n",
        "7. Plots the Precision-Recall Curve using seaborn & matplotlib.\n",
        "\n",
        "**Expected Output**\n",
        "* Precision-Recall Curve\n",
        "  * The higher the area under the curve, the better the classifier at distinguishing positive cases.\n",
        "  * A steep curve close to (1,1) indicates better performance.\n",
        "  * A PR AUC > 0.9 is generally excellent."
      ],
      "metadata": {
        "id": "u52jseoYypO2"
      }
    }
  ]
}